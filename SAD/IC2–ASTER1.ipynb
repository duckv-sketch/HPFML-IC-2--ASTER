{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b8c4055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# pruning_stage2_sad.py  (Stage-2 for SAD PFML)\n",
    "# ICÂ²â€“ASTER FFN neuron pruning (Transformer FFN filter pruning)\n",
    "# + PFML retraining (same PFML structure as your Stage-1)\n",
    "#\n",
    "# - Load data from SAD npz cache\n",
    "# - Load Stage-1 checkpoints (global/hyper/embed)\n",
    "# - Print params before/after pruning\n",
    "# - Prune FFN neurons with ICÂ² (capacity+independence) + ASTER sensitivity\n",
    "# - Retrain PFML for 50 rounds (configurable)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict, Counter\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "\n",
    "SEED = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dcd87bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# SAD cache\n",
    "SAVE_NPZ_CACHE = True\n",
    "CACHE_PATH = r\"E:\\Duc\\Safety Driving (2)\\Safety Driving\\SAD\\SAD_cache_paper_hp1.0_lp50.0_fs128_t-3.0_0.0_aug1x1_minAfterAug0.npz\"\n",
    "\n",
    "SAD_C = 33\n",
    "SAD_T = 385\n",
    "\n",
    "# PFML / train\n",
    "BATCH_SIZE = 64\n",
    "QR_SPLIT_RATE = 0.6\n",
    "\n",
    "\n",
    "# PFML stage-2 retrain rounds\n",
    "NUM_ROUNDS_STAGE2 = 100  # <-- báº¡n muá»‘n 50 rounds á»Ÿ stage-2\n",
    "INNER_LR = 1e-3\n",
    "OUTTER_LR = 1e-3\n",
    "INNER_STEP = 5\n",
    "OUTTER_STEP = 1\n",
    "P_RATE = 0.6\n",
    "\n",
    "\n",
    "# model\n",
    "D_MODEL = 64\n",
    "N_HEAD = 4\n",
    "N_LAYERS = 5\n",
    "\n",
    "# hypernet\n",
    "EMBED_DIM = 16\n",
    "HIDDEN_DIM = 128\n",
    "# HYPER_LR = 1e-3\n",
    "# EMBED_LR = 1e-3\n",
    "HYPER_LR = 5e-4\n",
    "EMBED_LR = 5e-4\n",
    "# hypernet supervision weight (server-side)\n",
    "LAMBDA_HYPER = 1.0\n",
    "# ------------------------------------------------------------\n",
    "# Pruning config\n",
    "# ------------------------------------------------------------\n",
    "PRUNE_RATIO = 0.50     # cáº¯t 50% FFN neurons\n",
    "SIGMA = 0.5            # ICÂ² mixing: capacity vs independence\n",
    "ALPHA = 1.0            # ASTER sensitivity exponent (>=0)\n",
    "K_NN = 5               # kNN for independence\n",
    "N_BINS = 32            # entropy bins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0029c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: use best\n",
    "# âœ… Stage-1 paper-style checkpoints (best by VAL/QUERY)\n",
    "CKPT_GLOBAL = \"PFML_bestVAL_global.pth\"\n",
    "CKPT_HYPER  = \"PFML_bestVAL_hypernet.pth\"\n",
    "CKPT_EMBED  = \"PFML_bestVAL_embed.pth\"\n",
    "CKPT_META   = \"PFML_bestVAL_meta1.json\"   # (optional) chá»©a best_p_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29289466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def compute_class_weights(subjects_Y):\n",
    "    from collections import Counter\n",
    "    import numpy as np\n",
    "\n",
    "    cnt = Counter()\n",
    "    for y in subjects_Y:\n",
    "        cnt.update(y.tolist())\n",
    "\n",
    "    total = cnt[0] + cnt[1]\n",
    "\n",
    "    # raw inverse-frequency\n",
    "    w0 = total / (2.0 * cnt[0])\n",
    "    w1 = total / (2.0 * cnt[1])\n",
    "\n",
    "    # ðŸ”¥ CORE FIX: temper the weights (sqrt)\n",
    "    w0 = np.sqrt(w0)\n",
    "    w1 = np.sqrt(w1)\n",
    "\n",
    "    print(f\"[Class Weight TEMPERED] class0={w0:.3f}, class1={w1:.3f}\")\n",
    "\n",
    "    return torch.tensor([w0, w1], dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4583383",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 1) Support / Query split\n",
    "# ============================================================\n",
    "def zscore_per_trial(X: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    X: (N, C, T)\n",
    "    Z-score per trial, per channel (normalize across time axis).\n",
    "    \"\"\"\n",
    "    mu = X.mean(axis=-1, keepdims=True)\n",
    "    sd = X.std(axis=-1, keepdims=True)\n",
    "    return (X - mu) / (sd + eps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8428baff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1) Train/Test + Support/Query split (paper-grade)\n",
    "def make_train_support_query_test_sets(subjects_X, subjects_Y,\n",
    "                                       train_rate=0.7, qr_split_rate=0.6,\n",
    "                                       do_zscore=True):\n",
    "    \"\"\"\n",
    "    Per subject:\n",
    "      - split into train/test (70/30)\n",
    "      - inside train: split into support/query (60/40)\n",
    "    Returns:\n",
    "      sup_sets, que_sets, test_sets (TensorDataset lists, aligned per subject)\n",
    "    \"\"\"\n",
    "    sup_sets, que_sets, test_sets = [], [], []\n",
    "\n",
    "    for X, Y in zip(subjects_X, subjects_Y):\n",
    "        n = len(X)\n",
    "        idx = np.arange(n)\n",
    "        np.random.shuffle(idx)\n",
    "\n",
    "        n_train = int(n * train_rate)\n",
    "        tr_idx = idx[:n_train]\n",
    "        te_idx = idx[n_train:]\n",
    "\n",
    "        Xtr, Ytr = X[tr_idx], Y[tr_idx]\n",
    "        Xte, Yte = X[te_idx], Y[te_idx]\n",
    "\n",
    "        # split train -> support/query\n",
    "        n_tr = len(Xtr)\n",
    "        tr2 = np.arange(n_tr)\n",
    "        np.random.shuffle(tr2)\n",
    "\n",
    "        n_sup = int(n_tr * qr_split_rate)\n",
    "        sup_idx = tr2[:n_sup]\n",
    "        que_idx = tr2[n_sup:]\n",
    "\n",
    "        Xsup, Ysup = Xtr[sup_idx], Ytr[sup_idx]\n",
    "        Xque, Yque = Xtr[que_idx], Ytr[que_idx]\n",
    "\n",
    "        # âœ… normalize (per trial, per channel)\n",
    "        if do_zscore:\n",
    "            Xsup = zscore_per_trial(Xsup).astype(np.float32)\n",
    "            Xque = zscore_per_trial(Xque).astype(np.float32)\n",
    "            Xte  = zscore_per_trial(Xte ).astype(np.float32)\n",
    "        else:\n",
    "            Xsup = Xsup.astype(np.float32)\n",
    "            Xque = Xque.astype(np.float32)\n",
    "            Xte  = Xte.astype(np.float32)\n",
    "\n",
    "        sup_sets.append(TensorDataset(torch.from_numpy(Xsup), torch.from_numpy(Ysup).long()))\n",
    "        que_sets.append(TensorDataset(torch.from_numpy(Xque), torch.from_numpy(Yque).long()))\n",
    "        test_sets.append(TensorDataset(torch.from_numpy(Xte ), torch.from_numpy(Yte ).long()))\n",
    "    \n",
    "    \n",
    "    return sup_sets, que_sets, test_sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85b0eeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2) Model (same as your Stage-1) + prunable FFN dim\n",
    "# ============================================================\n",
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(d_model // 2, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        w = self.attn(x)                # (B,T,1)\n",
    "        alpha = torch.softmax(w, dim=1) # (B,T,1)\n",
    "        return (x * alpha).sum(dim=1)   # (B,D)\n",
    "\n",
    "class EEGTransformerPrunable(nn.Module):\n",
    "    \"\"\"\n",
    "    Exactly your EEGTransformer, but allow ff_dim to change.\n",
    "    \"\"\"\n",
    "    def __init__(self, C=33, T=250, d_model=64, nhead=4, num_layers=6, ff_dim=128, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.C = C\n",
    "        self.T = T\n",
    "        self.d_model = d_model\n",
    "        self.ff_dim = ff_dim\n",
    "\n",
    "        self.spatial_conv = nn.Conv1d(C, C, kernel_size=1, groups=1, bias=False)\n",
    "        self.temporal_conv = nn.Conv1d(C, C, kernel_size=7, padding=3, groups=C, bias=False)\n",
    "\n",
    "        self.proj = nn.Linear(C, d_model)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers)\n",
    "\n",
    "        self.temp_attn = TemporalAttention(d_model)\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.size(1) != self.C:\n",
    "            raise RuntimeError(f\"[EEGTransformer] Channel mismatch: got {x.size(1)}, expected {self.C}\")\n",
    "\n",
    "        x = self.spatial_conv(x)\n",
    "        x = self.temporal_conv(x)\n",
    "\n",
    "        x = x.permute(0, 2, 1)   # (B,T,C)\n",
    "        h = self.proj(x)         # (B,T,D)\n",
    "        h = self.encoder(h)      # (B,T,D)\n",
    "        h = self.temp_attn(h)    # (B,D)\n",
    "        return self.classifier(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "462fcd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3) ICÂ²â€“ASTER score for FFN neurons\n",
    "#    v_j = [W1[j,:], W2[:,j]^T] âˆˆ R^{2d}\n",
    "#    IMcap: 1 - entropy(|v|)\n",
    "#    IMind: kNN distance\n",
    "#    ASTER gate: g_j = ||v||^2, score = (Ïƒ cap + (1-Ïƒ) ind) * (g_j)^(Î±)\n",
    "# ============================================================\n",
    "def entropy_capacity(v: torch.Tensor, n_bins=32):\n",
    "    v = v.abs()\n",
    "    mn, mx = v.min().item(), v.max().item()\n",
    "    if mx - mn < 1e-12:\n",
    "        return 0.0\n",
    "    hist = torch.histc(v, bins=n_bins, min=mn, max=mx)\n",
    "    p = hist / (hist.sum() + 1e-12)\n",
    "    H = -(p * (p + 1e-12).log2()).sum().item()\n",
    "    H_norm = H / (np.log2(n_bins) + 1e-12)\n",
    "    return 1.0 - H_norm\n",
    "\n",
    "@torch.no_grad()\n",
    "def ic2_aster_scores(linear1: nn.Linear, linear2: nn.Linear,\n",
    "                     sigma=0.5, alpha=1.0, k_nn=5, n_bins=32):\n",
    "    W1 = linear1.weight.detach().float()      # (ff, d)\n",
    "    W2 = linear2.weight.detach().float()      # (d, ff)\n",
    "    ff = W1.shape[0]\n",
    "    d  = W1.shape[1]\n",
    "\n",
    "    V = torch.cat([W1, W2.t()], dim=1)        # (ff, 2d)\n",
    "\n",
    "    # IMcap\n",
    "    cap = torch.zeros(ff, device=V.device)\n",
    "    for j in range(ff):\n",
    "        cap[j] = entropy_capacity(V[j], n_bins=n_bins)\n",
    "\n",
    "    # IMind (kNN distance in weight space)\n",
    "    dist = torch.cdist(V, V, p=2)\n",
    "    dist.fill_diagonal_(float(\"inf\"))\n",
    "    k = min(k_nn, max(1, ff - 1))\n",
    "    nn_d = torch.topk(dist, k=k, largest=False).values\n",
    "    ind = nn_d.mean(dim=1)\n",
    "\n",
    "    # normalize to [0,1]\n",
    "    cap_n = (cap - cap.min()) / (cap.max() - cap.min() + 1e-12)\n",
    "    ind_n = (ind - ind.min()) / (ind.max() - ind.min() + 1e-12)\n",
    "\n",
    "    O = sigma * cap_n + (1 - sigma) * ind_n\n",
    "\n",
    "    # ASTER sensitivity gate\n",
    "    g = (V.norm(p=2, dim=1) ** 2)  # ||v||_2^2\n",
    "    score = O * (g ** alpha)\n",
    "    return score  # (ff,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1a7b234",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 4) Prune Transformer FFN neurons and rebuild smaller model\n",
    "# ============================================================\n",
    "@torch.no_grad()\n",
    "def prune_ffn_neurons(model_full: EEGTransformerPrunable, prune_ratio=0.5,\n",
    "                      sigma=0.5, alpha=1.0, k_nn=5, n_bins=32):\n",
    "    device = next(model_full.parameters()).device\n",
    "    C = model_full.C\n",
    "    T = model_full.T\n",
    "    d_model = model_full.d_model\n",
    "    nhead = model_full.encoder.layers[0].self_attn.num_heads\n",
    "    num_layers = len(model_full.encoder.layers)\n",
    "\n",
    "    ff_full = model_full.encoder.layers[0].linear1.out_features\n",
    "    ff_keep = max(1, int(np.floor((1 - prune_ratio) * ff_full)))\n",
    "\n",
    "    model_small = EEGTransformerPrunable(\n",
    "        C=C, T=T, d_model=d_model, nhead=nhead,\n",
    "        num_layers=num_layers, ff_dim=ff_keep, num_classes=2\n",
    "    ).to(device)\n",
    "\n",
    "    # copy conv/proj/temp_attn/classifier\n",
    "    model_small.spatial_conv.load_state_dict(model_full.spatial_conv.state_dict(), strict=True)\n",
    "    model_small.temporal_conv.load_state_dict(model_full.temporal_conv.state_dict(), strict=True)\n",
    "    model_small.proj.load_state_dict(model_full.proj.state_dict(), strict=True)\n",
    "    model_small.temp_attn.load_state_dict(model_full.temp_attn.state_dict(), strict=True)\n",
    "    model_small.classifier.load_state_dict(model_full.classifier.state_dict(), strict=True)\n",
    "\n",
    "    # copy each encoder layer (attention+norm keep, FFN prune)\n",
    "    for li in range(num_layers):\n",
    "        Lf = model_full.encoder.layers[li]\n",
    "        Ls = model_small.encoder.layers[li]\n",
    "\n",
    "        # attention & norms\n",
    "        Ls.self_attn.load_state_dict(Lf.self_attn.state_dict(), strict=True)\n",
    "        Ls.norm1.load_state_dict(Lf.norm1.state_dict(), strict=True)\n",
    "        Ls.norm2.load_state_dict(Lf.norm2.state_dict(), strict=True)\n",
    "\n",
    "        # FFN prune\n",
    "        scores = ic2_aster_scores(Lf.linear1, Lf.linear2,\n",
    "                                  sigma=sigma, alpha=alpha, k_nn=k_nn, n_bins=n_bins)\n",
    "        keep_idx = torch.topk(scores, k=ff_keep, largest=True).indices.sort()[0]\n",
    "\n",
    "        # linear1: (ff_keep, d_model)\n",
    "        Ls.linear1.weight.copy_(Lf.linear1.weight[keep_idx])\n",
    "        Ls.linear1.bias.copy_(Lf.linear1.bias[keep_idx])\n",
    "\n",
    "        # linear2: (d_model, ff_keep)\n",
    "        Ls.linear2.weight.copy_(Lf.linear2.weight[:, keep_idx])\n",
    "        Ls.linear2.bias.copy_(Lf.linear2.bias)\n",
    "\n",
    "    return model_small\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbb3f17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 5) Hypernet parts (same logic as your Stage-1)\n",
    "# ============================================================\n",
    "class Embed(nn.Module):\n",
    "    def __init__(self, client_num, emb_dim):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(client_num, emb_dim)\n",
    "    def forward(self, client_id):\n",
    "        if not torch.is_tensor(client_id):\n",
    "            client_id = torch.tensor([client_id], dtype=torch.long, device=self.embed.weight.device)\n",
    "        return self.embed(client_id)\n",
    "\n",
    "class HeadHyperNet(nn.Module):\n",
    "    def __init__(self, embed: Embed, emb_dim=16, hidden_dim=128, d_model=64, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.embed = embed\n",
    "        self.num_classes = num_classes\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.w_gen = nn.Linear(hidden_dim, num_classes * d_model)\n",
    "        self.b_gen = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, client_id):\n",
    "        e = self.embed(client_id)\n",
    "        h = self.mlp(e)\n",
    "        w = self.w_gen(h).view(self.num_classes, self.d_model)\n",
    "        # constrained (same as your fix)\n",
    "        w = w / (w.norm(p=2, dim=1, keepdim=True) + 1e-6)\n",
    "        b = self.b_gen(h).view(self.num_classes)\n",
    "        b = torch.clamp(b, -1.0, 1.0)\n",
    "        return OrderedDict({\"classifier.weight\": w, \"classifier.bias\": b})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8d16292",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 6) PFML correct evaluation\n",
    "# ============================================================\n",
    "@torch.no_grad()\n",
    "def eval_intra_full_metrics_pfml(trainer, p_rate=None, save_txt=False, out_path=\"PFML_stage2_intra_results.txt\"):\n",
    "    if p_rate is None:\n",
    "        p_rate = trainer.p_rate\n",
    "\n",
    "    global_state = copy.deepcopy(trainer.global_model.state_dict())\n",
    "    y_true_all, y_pred_all = [], []\n",
    "\n",
    "    for c in trainer.clients:\n",
    "        cid = c.cid\n",
    "        zeta = trainer.hypernet(torch.tensor([cid], dtype=torch.long, device=DEVICE))\n",
    "\n",
    "        theta = copy.deepcopy(global_state)\n",
    "        theta[\"classifier.weight\"] = (1.0 - p_rate) * theta[\"classifier.weight\"] + p_rate * zeta[\"classifier.weight\"]\n",
    "        theta[\"classifier.bias\"]   = (1.0 - p_rate) * theta[\"classifier.bias\"]   + p_rate * zeta[\"classifier.bias\"]\n",
    "\n",
    "        model = trainer.model_ctor().to(DEVICE)\n",
    "        model.load_state_dict(theta, strict=True)\n",
    "        model.eval()\n",
    "\n",
    "        for xb, yb in c.que_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            pred = logits.argmax(1).cpu().numpy()\n",
    "            y_true_all.append(yb.numpy())\n",
    "            y_pred_all.append(pred)\n",
    "\n",
    "    y_true_all = np.concatenate(y_true_all)\n",
    "    y_pred_all = np.concatenate(y_pred_all)\n",
    "\n",
    "    acc = accuracy_score(y_true_all, y_pred_all)\n",
    "    macro_f1 = f1_score(y_true_all, y_pred_all, average=\"macro\")\n",
    "    cm = confusion_matrix(y_true_all, y_pred_all, labels=[0, 1])\n",
    "\n",
    "    print(\"\\n================ PFML-CORRECT INTRA EVAL (Stage-2) ================\")\n",
    "    print(f\"Accuracy  : {acc*100:.2f}%\")\n",
    "    print(f\"Macro-F1  : {macro_f1*100:.2f}%\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true_all, y_pred_all, digits=4, zero_division=0))\n",
    "    print(\"===================================================================\\n\")\n",
    "\n",
    "    if save_txt:\n",
    "        with open(out_path, \"w\") as f:\n",
    "            f.write(f\"Accuracy  : {acc*100:.2f}%\\n\")\n",
    "            f.write(f\"Macro-F1  : {macro_f1*100:.2f}%\\n\")\n",
    "            f.write(\"Confusion Matrix:\\n\")\n",
    "            f.write(str(cm) + \"\\n\\n\")\n",
    "            f.write(classification_report(y_true_all, y_pred_all, digits=4, zero_division=0))\n",
    "\n",
    "    return acc, macro_f1, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "298962c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def _safe_name(s: str) -> str:\n",
    "    return re.sub(r\"[^a-zA-Z0-9._-]+\", \"_\", str(s))\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_intra_per_subject_pfml(\n",
    "    trainer,\n",
    "    p_rate=None,\n",
    "    save_txt=False,\n",
    "    print_per_subject=True,\n",
    "    tag=\"VAL\",\n",
    "    use_eval_loader=True,\n",
    "    out_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    PFML intra eval (QUERY):\n",
    "    - personalized model per client (subject)\n",
    "    - metrics per subject\n",
    "    - report MeanÂ±Std over subjects + micro reference\n",
    "    - if use_eval_loader=True: dÃ¹ng que_eval_loader (shuffle=False, drop_last=False)\n",
    "    \"\"\"\n",
    "    if p_rate is None:\n",
    "        p_rate = trainer.p_rate\n",
    "\n",
    "    global_state = copy.deepcopy(trainer.global_model.state_dict())\n",
    "\n",
    "    per_sub = []\n",
    "    y_true_micro, y_pred_micro = [], []\n",
    "\n",
    "    for c in trainer.clients:\n",
    "        cid = c.cid\n",
    "\n",
    "        # hyper head\n",
    "        zeta = trainer.hypernet(torch.tensor([cid], dtype=torch.long, device=DEVICE))\n",
    "\n",
    "        # personalized theta\n",
    "        theta = copy.deepcopy(global_state)\n",
    "        theta[\"classifier.weight\"] = (1.0 - p_rate) * theta[\"classifier.weight\"] + p_rate * zeta[\"classifier.weight\"]\n",
    "        theta[\"classifier.bias\"]   = (1.0 - p_rate) * theta[\"classifier.bias\"]   + p_rate * zeta[\"classifier.bias\"]\n",
    "\n",
    "        model = trainer.model_ctor().to(DEVICE)\n",
    "        model.load_state_dict(theta, strict=True)\n",
    "        model.eval()\n",
    "\n",
    "        loader = c.que_eval_loader if use_eval_loader else c.que_loader\n",
    "\n",
    "        ys, ps = [], []\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            pred = logits.argmax(1).cpu().numpy()\n",
    "            ys.append(yb.numpy())\n",
    "            ps.append(pred)\n",
    "\n",
    "        ys = np.concatenate(ys) if len(ys) else np.array([], dtype=np.int64)\n",
    "        ps = np.concatenate(ps) if len(ps) else np.array([], dtype=np.int64)\n",
    "        if len(ys) == 0:\n",
    "            continue\n",
    "\n",
    "        acc_i = accuracy_score(ys, ps)\n",
    "        f1_i  = f1_score(ys, ps, average=\"macro\")\n",
    "        cm_i  = confusion_matrix(ys, ps, labels=[0, 1])\n",
    "\n",
    "        per_sub.append({\"cid\": cid, \"n\": len(ys), \"acc\": acc_i, \"macro_f1\": f1_i, \"cm\": cm_i})\n",
    "\n",
    "        y_true_micro.append(ys)\n",
    "        y_pred_micro.append(ps)\n",
    "\n",
    "    # macro over subjects\n",
    "    acc_list = np.array([d[\"acc\"] for d in per_sub], dtype=np.float32)\n",
    "    f1_list  = np.array([d[\"macro_f1\"] for d in per_sub], dtype=np.float32)\n",
    "\n",
    "    if len(acc_list) > 1:\n",
    "        acc_mean, acc_std = acc_list.mean(), acc_list.std(ddof=1)\n",
    "    else:\n",
    "        acc_mean, acc_std = (acc_list.mean() if len(acc_list) else 0.0), 0.0\n",
    "\n",
    "    if len(f1_list) > 1:\n",
    "        f1_mean, f1_std = f1_list.mean(), f1_list.std(ddof=1)\n",
    "    else:\n",
    "        f1_mean, f1_std = (f1_list.mean() if len(f1_list) else 0.0), 0.0\n",
    "\n",
    "    # micro overall (reference)\n",
    "    y_true_micro = np.concatenate(y_true_micro) if len(y_true_micro) else np.array([], dtype=np.int64)\n",
    "    y_pred_micro = np.concatenate(y_pred_micro) if len(y_pred_micro) else np.array([], dtype=np.int64)\n",
    "\n",
    "    micro_acc = accuracy_score(y_true_micro, y_pred_micro) if len(y_true_micro) else 0.0\n",
    "    micro_f1  = f1_score(y_true_micro, y_pred_micro, average=\"macro\") if len(y_true_micro) else 0.0\n",
    "\n",
    "    print(f\"\\n================ PFML INTRA ({tag}) ================\")\n",
    "    print(f\"Subjects used : {len(per_sub)}\")\n",
    "    print(f\"Mean ACC      : {acc_mean*100:.2f}%  (Std {acc_std*100:.2f}%)\")\n",
    "    print(f\"Mean Macro-F1 : {f1_mean*100:.2f}%  (Std {f1_std*100:.2f}%)\")\n",
    "   \n",
    "    print(\"====================================================\\n\")\n",
    "\n",
    "    if print_per_subject:\n",
    "        print(\"Per-subject results:\")\n",
    "        for d in per_sub:\n",
    "            print(f\"  cid={d['cid']:>2} | N={d['n']:>5} | ACC={d['acc']*100:>6.2f}% | Macro-F1={d['macro_f1']*100:>6.2f}%\")\n",
    "\n",
    "    if save_txt:\n",
    "        if out_path is None:\n",
    "            out_path = f\"PFML_intra_{_safe_name(tag)}.txt\"\n",
    "        with open(out_path, \"w\") as f:\n",
    "            f.write(f\"Subjects used : {len(per_sub)}\\n\")\n",
    "            f.write(f\"Mean ACC      : {acc_mean*100:.2f}% (Std {acc_std*100:.2f}%)\\n\")\n",
    "            f.write(f\"Mean Macro-F1 : {f1_mean*100:.2f}% (Std {f1_std*100:.2f}%)\\n\")\n",
    "            f.write(f\"Micro ACC     : {micro_acc*100:.2f}%\\n\")\n",
    "            f.write(f\"Micro Macro-F1: {micro_f1*100:.2f}%\\n\\n\")\n",
    "            for d in per_sub:\n",
    "                f.write(f\"cid={d['cid']} N={d['n']} ACC={d['acc']*100:.2f} Macro-F1={d['macro_f1']*100:.2f}\\n\")\n",
    "\n",
    "    return per_sub, (acc_mean, acc_std), (f1_mean, f1_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "527fe992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6) Client: meta-train with support/query (inner/outer)\n",
    "# ============================================================\n",
    "class MetaClient:\n",
    "    def __init__(self, cid, innerlr, outterlr,\n",
    "                 inner_step, outter_step,\n",
    "                 batch_size, class_weights):\n",
    "        self.cid = cid\n",
    "        self.innerlr = innerlr\n",
    "        self.outterlr = outterlr\n",
    "        self.inner_step = inner_step\n",
    "        self.outter_step = outter_step\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "        self.sup_loader = None\n",
    "        self.que_loader = None\n",
    "        self.test_loader = None\n",
    "\n",
    "        # eval loaders (no shuffle / no drop_last)\n",
    "        self.que_eval_loader = None\n",
    "        self.test_eval_loader = None\n",
    "\n",
    "    def init_dataset(self, sup_set, que_set, test_set):\n",
    "        # train-time loaders\n",
    "        self.sup_loader = DataLoader(sup_set, batch_size=self.batch_size, shuffle=True, drop_last=True)\n",
    "        self.que_loader = DataLoader(que_set, batch_size=self.batch_size, shuffle=True, drop_last=False)\n",
    "        self.test_loader = DataLoader(test_set, batch_size=self.batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "        # eval loaders (full)\n",
    "        self.que_eval_loader = DataLoader(que_set, batch_size=self.batch_size, shuffle=False, drop_last=False)\n",
    "        self.test_eval_loader = self.test_loader\n",
    "\n",
    "    @staticmethod\n",
    "    def _next_batch(loader, it_state):\n",
    "        try:\n",
    "            batch = next(it_state[\"it\"])\n",
    "        except StopIteration:\n",
    "            it_state[\"it\"] = iter(loader)\n",
    "            batch = next(it_state[\"it\"])\n",
    "        return batch\n",
    "\n",
    "    def train(self, hyper_head, global_state, p_rate, model_ctor):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          outer_state (for FedAvg global update),\n",
    "          ft_head_target (for hypernet supervision),\n",
    "          inner_acc/loss, outer_acc/loss\n",
    "        \"\"\"\n",
    "\n",
    "        # (1) mix global head with hyper head\n",
    "        theta = copy.deepcopy(global_state)\n",
    "        gw = theta[\"classifier.weight\"]\n",
    "        gb = theta[\"classifier.bias\"]\n",
    "        theta[\"classifier.weight\"] = (1 - p_rate) * gw + p_rate * hyper_head[\"classifier.weight\"]\n",
    "        theta[\"classifier.bias\"]   = (1 - p_rate) * gb + p_rate * hyper_head[\"classifier.bias\"]\n",
    "\n",
    "        # ---------------- INNER: support fine-tune ----------------\n",
    "        fast_model = model_ctor().to(DEVICE)\n",
    "        fast_model.load_state_dict(theta, strict=True)\n",
    "        inner_opt = torch.optim.Adam(fast_model.parameters(), lr=self.innerlr)\n",
    "\n",
    "        sup_it = {\"it\": iter(self.sup_loader)}\n",
    "        total_loss, total_correct, total_sample = 0, 0, 0\n",
    "\n",
    "        for _ in range(self.inner_step):\n",
    "            xb, yb = self._next_batch(self.sup_loader, sup_it)\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "\n",
    "            inner_opt.zero_grad()\n",
    "            logits = fast_model(xb)\n",
    "            loss = self.criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            inner_opt.step()\n",
    "\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            total_correct += (logits.argmax(1) == yb).sum().item()\n",
    "            total_sample += xb.size(0)\n",
    "\n",
    "        inner_acc  = total_correct / max(1, total_sample)\n",
    "        inner_loss = total_loss / max(1, total_sample)\n",
    "\n",
    "        fast_state = copy.deepcopy(fast_model.state_dict())\n",
    "\n",
    "        # target head for hypernet supervision (detach)\n",
    "        ft_head_target = {\n",
    "            \"classifier.weight\": fast_state[\"classifier.weight\"].detach().clone(),\n",
    "            \"classifier.bias\":   fast_state[\"classifier.bias\"].detach().clone()\n",
    "        }\n",
    "\n",
    "        # ---------------- OUTER: query step from fast weights ----------------\n",
    "        # IMPORTANT: start from fast_model (after inner), not from theta\n",
    "        outer_model = model_ctor().to(DEVICE)\n",
    "        outer_model.load_state_dict(fast_state, strict=True)\n",
    "        out_opt = torch.optim.Adam(outer_model.parameters(), lr=self.outterlr)\n",
    "\n",
    "        que_it = {\"it\": iter(self.que_loader)}\n",
    "        total_loss, total_correct, total_sample = 0, 0, 0\n",
    "\n",
    "        for _ in range(self.outter_step):\n",
    "            xb, yb = self._next_batch(self.que_loader, que_it)\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "\n",
    "            out_opt.zero_grad()\n",
    "            logits = outer_model(xb)\n",
    "            loss = self.criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            out_opt.step()\n",
    "\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            total_correct += (logits.argmax(1) == yb).sum().item()\n",
    "            total_sample += xb.size(0)\n",
    "\n",
    "        out_acc  = total_correct / max(1, total_sample)\n",
    "        out_loss = total_loss / max(1, total_sample)\n",
    "\n",
    "        outer_state = copy.deepcopy(outer_model.state_dict())\n",
    "        return outer_state, ft_head_target, inner_acc, inner_loss, out_acc, out_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ab82e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_test_per_subject_pfml(\n",
    "    trainer,\n",
    "    p_rate=None,\n",
    "    save_txt=False,\n",
    "    tag=\"TEST\",\n",
    "    print_per_subject=True,\n",
    "    print_cm_per_subject=False,\n",
    "    out_path=\"PFML_test_detail.txt\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate on held-out TEST split (per subject), using personalized head mixing.\n",
    "    Reports meanÂ±std over subjects + overall confusion matrix.\n",
    "    \"\"\"\n",
    "    if p_rate is None:\n",
    "        p_rate = trainer.p_rate\n",
    "\n",
    "    global_state = copy.deepcopy(trainer.global_model.state_dict())\n",
    "\n",
    "    per_sub = []\n",
    "    y_true_all, y_pred_all = [], []\n",
    "\n",
    "    for c in trainer.clients:\n",
    "        cid = c.cid\n",
    "        zeta = trainer.hypernet(torch.tensor([cid], dtype=torch.long, device=DEVICE))\n",
    "\n",
    "        theta = copy.deepcopy(global_state)\n",
    "        theta[\"classifier.weight\"] = (1.0 - p_rate) * theta[\"classifier.weight\"] + p_rate * zeta[\"classifier.weight\"]\n",
    "        theta[\"classifier.bias\"]   = (1.0 - p_rate) * theta[\"classifier.bias\"]   + p_rate * zeta[\"classifier.bias\"]\n",
    "\n",
    "        model = trainer.model_ctor().to(DEVICE)\n",
    "        model.load_state_dict(theta, strict=True)\n",
    "        model.eval()\n",
    "\n",
    "        ys, ps = [], []\n",
    "        for xb, yb in c.test_eval_loader:  # shuffle=False, drop_last=False\n",
    "            xb = xb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            pred = logits.argmax(1).cpu().numpy()\n",
    "            ys.append(yb.numpy())\n",
    "            ps.append(pred)\n",
    "\n",
    "        ys = np.concatenate(ys) if len(ys) else np.array([], dtype=np.int64)\n",
    "        ps = np.concatenate(ps) if len(ps) else np.array([], dtype=np.int64)\n",
    "        if len(ys) == 0:\n",
    "            continue\n",
    "\n",
    "        acc_i = accuracy_score(ys, ps)\n",
    "        f1_i  = f1_score(ys, ps, average=\"macro\")\n",
    "        cm_i  = confusion_matrix(ys, ps, labels=[0, 1])\n",
    "\n",
    "        per_sub.append({\"cid\": cid, \"n\": len(ys), \"acc\": acc_i, \"macro_f1\": f1_i, \"cm\": cm_i})\n",
    "\n",
    "        y_true_all.append(ys)\n",
    "        y_pred_all.append(ps)\n",
    "\n",
    "    # macro over subjects\n",
    "    acc_list = np.array([d[\"acc\"] for d in per_sub], dtype=np.float32)\n",
    "    f1_list  = np.array([d[\"macro_f1\"] for d in per_sub], dtype=np.float32)\n",
    "\n",
    "    acc_mean = acc_list.mean() if len(acc_list) else 0.0\n",
    "    acc_std  = acc_list.std(ddof=1) if len(acc_list) > 1 else 0.0\n",
    "    f1_mean  = f1_list.mean() if len(f1_list) else 0.0\n",
    "    f1_std   = f1_list.std(ddof=1) if len(f1_list) > 1 else 0.0\n",
    "\n",
    "    y_true_all = np.concatenate(y_true_all) if len(y_true_all) else np.array([], dtype=np.int64)\n",
    "    y_pred_all = np.concatenate(y_pred_all) if len(y_pred_all) else np.array([], dtype=np.int64)\n",
    "    cm = confusion_matrix(y_true_all, y_pred_all, labels=[0, 1]) if len(y_true_all) else np.zeros((2, 2), dtype=int)\n",
    "\n",
    "    print(f\"\\n================ PFML {tag} (TEST) ================\")\n",
    "    print(f\"Subjects used : {len(per_sub)}\")\n",
    "    print(f\"Mean ACC      : {acc_mean*100:.2f}%  (Std {acc_std*100:.2f}%)\")\n",
    "    print(f\"Mean Macro-F1 : {f1_mean*100:.2f}%  (Std {f1_std*100:.2f}%)\")\n",
    "    print(\"Overall Confusion Matrix (all test samples):\")\n",
    "    print(cm)\n",
    "\n",
    "    if print_per_subject:\n",
    "        print(\"\\nPer-subject results:\")\n",
    "        print(\"  cid |     N |   ACC   | Macro-F1\")\n",
    "        print(\"------|-------|---------|---------\")\n",
    "        for d in per_sub:\n",
    "            print(f\"  {d['cid']:>3} | {d['n']:>5} | {d['acc']*100:>6.2f}% | {d['macro_f1']*100:>7.2f}%\")\n",
    "\n",
    "        if print_cm_per_subject:\n",
    "            print(\"\\nPer-subject Confusion Matrices:\")\n",
    "            for d in per_sub:\n",
    "                print(f\"[cid={d['cid']}] cm=\\n{d['cm']}\\n\")\n",
    "\n",
    "    print(\"====================================================\\n\")\n",
    "\n",
    "    if save_txt:\n",
    "        with open(out_path, \"w\") as f:\n",
    "            f.write(f\"Subjects used : {len(per_sub)}\\n\")\n",
    "            f.write(f\"Mean ACC      : {acc_mean*100:.2f}% (Std {acc_std*100:.2f}%)\\n\")\n",
    "            f.write(f\"Mean Macro-F1 : {f1_mean*100:.2f}% (Std {f1_std*100:.2f}%)\\n\")\n",
    "            f.write(\"Overall Confusion Matrix:\\n\")\n",
    "            f.write(str(cm) + \"\\n\\n\")\n",
    "            f.write(\"Per-subject:\\n\")\n",
    "            for d in per_sub:\n",
    "                f.write(f\"cid={d['cid']} N={d['n']} ACC={d['acc']*100:.2f} Macro-F1={d['macro_f1']*100:.2f}\\n\")\n",
    "                if print_cm_per_subject:\n",
    "                    f.write(f\"cm=\\n{d['cm']}\\n\\n\")\n",
    "\n",
    "    return per_sub, (acc_mean, acc_std), (f1_mean, f1_std), cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8956fef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PFML Trainer (Stage-2) - paper style like Stage-1\n",
    "#   - Select best by VAL(QUERY)\n",
    "#   - TEST only once at the end (outside trainer)\n",
    "#   - Hypernet supervision via MSE on classifier head\n",
    "# ============================================================\n",
    "class PFMLTrainer:\n",
    "    def __init__(self, num_client, batch_size, qr_split_rate,\n",
    "                 innerlr, outterlr, innerstep, outterstep, p_rate,\n",
    "                 ff_dim, class_weights,\n",
    "                 init_global_state=None,\n",
    "                 init_hyper_state=None,\n",
    "                 init_embed_state=None):\n",
    "\n",
    "        self.num_client = num_client\n",
    "        self.batch_size = batch_size\n",
    "        self.qr_split_rate = qr_split_rate\n",
    "\n",
    "        self.innerlr = innerlr\n",
    "        self.outterlr = outterlr\n",
    "        self.innerstep = innerstep\n",
    "        self.outterstep = outterstep\n",
    "        self.p_rate = p_rate\n",
    "\n",
    "        self.class_weights = class_weights\n",
    "        self.ff_dim = ff_dim\n",
    "\n",
    "        # pruned backbone ctor\n",
    "        self.model_ctor = lambda: EEGTransformerPrunable(\n",
    "            C=SAD_C, T=SAD_T, d_model=D_MODEL, nhead=N_HEAD, num_layers=N_LAYERS,\n",
    "            ff_dim=self.ff_dim, num_classes=2\n",
    "        )\n",
    "\n",
    "        # global model\n",
    "        self.global_model = self.model_ctor().to(DEVICE)\n",
    "        if init_global_state is not None:\n",
    "            self.global_model.load_state_dict(init_global_state, strict=True)\n",
    "\n",
    "        # hypernet + embed\n",
    "        self.embed = Embed(self.num_client, EMBED_DIM).to(DEVICE)\n",
    "        self.hypernet = HeadHyperNet(self.embed, EMBED_DIM, HIDDEN_DIM, D_MODEL, 2).to(DEVICE)\n",
    "\n",
    "        if init_embed_state is not None:\n",
    "            self.embed.load_state_dict(init_embed_state, strict=True)\n",
    "        if init_hyper_state is not None:\n",
    "            self.hypernet.load_state_dict(init_hyper_state, strict=True)\n",
    "\n",
    "        # optimizers (same as stage-1)\n",
    "        self.hyper_opt = torch.optim.Adam(\n",
    "            [p for n, p in self.hypernet.named_parameters() if \"embed\" not in n],\n",
    "            lr=HYPER_LR\n",
    "        )\n",
    "        self.embed_opt = torch.optim.Adam(self.embed.parameters(), lr=EMBED_LR)\n",
    "\n",
    "        self.clients = []\n",
    "\n",
    "        # ============================================================\n",
    "        # [ADD] history for plotting (round-wise)\n",
    "        # ============================================================\n",
    "        self.history = {\n",
    "            \"round\": [],\n",
    "            \"p_rate\": [],\n",
    "            \"val_acc_mean\": [],\n",
    "            \"val_acc_std\": [],\n",
    "            \"val_f1_mean\": [],\n",
    "            \"val_f1_std\": [],\n",
    "            \"hyper_loss\": [],        # mean hyper loss per round\n",
    "            \"inner_loss_mean\": [],   # mean over clients per round\n",
    "            \"outer_loss_mean\": [],   # mean over clients per round\n",
    "            \"inner_acc_mean\": [],    # optional\n",
    "            \"outer_acc_mean\": [],    # optional\n",
    "        }\n",
    "\n",
    "    def init_clients(self, sup_sets, que_sets, test_sets):\n",
    "        self.clients = []\n",
    "        for cid in range(self.num_client):\n",
    "            c = MetaClient(\n",
    "                cid,\n",
    "                self.innerlr,\n",
    "                self.outterlr,\n",
    "                self.innerstep,\n",
    "                self.outterstep,\n",
    "                self.batch_size,\n",
    "                self.class_weights\n",
    "            )\n",
    "            c.init_dataset(sup_sets[cid], que_sets[cid], test_sets[cid])\n",
    "            self.clients.append(c)\n",
    "        print(f\"[PFML] Created {len(self.clients)} clients.\")\n",
    "\n",
    "    def fedavg(self, model_states):\n",
    "        avg = OrderedDict()\n",
    "        for k in model_states[0].keys():\n",
    "            avg[k] = sum(ms[k] for ms in model_states) / len(model_states)\n",
    "        return avg\n",
    "\n",
    "    def run(self, num_rounds, save_prefix=\"PFML_stage2\"):\n",
    "        print(f\"[PFML Stage-2] Start training on {DEVICE}, rounds={num_rounds}, ff_dim={self.ff_dim}\")\n",
    "\n",
    "        best_val_acc = 0.0\n",
    "        best_round = 0\n",
    "        best_p_rate = None\n",
    "\n",
    "        mse = nn.MSELoss()\n",
    "\n",
    "        for rnd in range(1, num_rounds + 1):\n",
    "            # schedule p_rate (same as stage-1)\n",
    "            p_rate_rnd = self.p_rate  # fixed\n",
    "\n",
    "\n",
    "            outer_states = []\n",
    "            global_state = copy.deepcopy(self.global_model.state_dict())\n",
    "\n",
    "            # -------- server-side hypernet supervised loss --------\n",
    "            hyper_loss_total = 0.0\n",
    "\n",
    "            # ============================================================\n",
    "            # [ADD] round-wise accumulators for inner/outer stats\n",
    "            # ============================================================\n",
    "            inner_loss_sum = 0.0\n",
    "            outer_loss_sum = 0.0\n",
    "            inner_acc_sum  = 0.0\n",
    "            outer_acc_sum  = 0.0\n",
    "            n_clients_used = 0\n",
    "\n",
    "            for c in self.clients:\n",
    "                cid_t = torch.tensor([c.cid], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "                # zeta_i from hypernet (keep graph)\n",
    "                zeta = self.hypernet(cid_t)\n",
    "\n",
    "                # client local meta-learning\n",
    "                outer_state, ft_head_target, inner_acc, inner_loss, out_acc, out_loss = c.train(\n",
    "                    zeta, global_state, p_rate_rnd, self.model_ctor\n",
    "                )\n",
    "                outer_states.append(outer_state)\n",
    "\n",
    "                # ============================================================\n",
    "                # [ADD] accumulate inner/outer stats\n",
    "                # ============================================================\n",
    "                inner_loss_sum += float(inner_loss)\n",
    "                outer_loss_sum += float(out_loss)\n",
    "                inner_acc_sum  += float(inner_acc)\n",
    "                outer_acc_sum  += float(out_acc)\n",
    "                n_clients_used += 1\n",
    "\n",
    "                # hypernet supervision\n",
    "                hyper_loss = mse(zeta[\"classifier.weight\"], ft_head_target[\"classifier.weight\"]) + \\\n",
    "                             mse(zeta[\"classifier.bias\"],   ft_head_target[\"classifier.bias\"])\n",
    "                hyper_loss_total = hyper_loss_total + hyper_loss\n",
    "\n",
    "            # (1) update global by FedAvg\n",
    "            self.global_model.load_state_dict(self.fedavg(outer_states), strict=True)\n",
    "\n",
    "            # (2) update hypernet + embed\n",
    "            self.hyper_opt.zero_grad()\n",
    "            self.embed_opt.zero_grad()\n",
    "            (LAMBDA_HYPER * hyper_loss_total / max(1, len(self.clients))).backward()\n",
    "            self.hyper_opt.step()\n",
    "            self.embed_opt.step()\n",
    "\n",
    "            # ============================================================\n",
    "            # [ADD] round-wise means for inner/outer\n",
    "            # ============================================================\n",
    "            inner_loss_mean = inner_loss_sum / max(1, n_clients_used)\n",
    "            outer_loss_mean = outer_loss_sum / max(1, n_clients_used)\n",
    "            inner_acc_mean  = inner_acc_sum  / max(1, n_clients_used)\n",
    "            outer_acc_mean  = outer_acc_sum  / max(1, n_clients_used)\n",
    "\n",
    "            # (3) VAL = QUERY (paper-grade model selection)\n",
    "            # âœ… per-subject giá»‘ng stage-1 -> dÃ¹ng que_eval_loader (shuffle=False)\n",
    "            per_sub, acc_stat, f1_stat = eval_intra_per_subject_pfml(\n",
    "                self,\n",
    "                p_rate=p_rate_rnd,\n",
    "                save_txt=False,\n",
    "                print_per_subject=True,   # <-- Ä‘á»•i False náº¿u muá»‘n gá»n log\n",
    "                tag=f\"VAL@Round{rnd:03d}\",\n",
    "                use_eval_loader=True\n",
    "            )\n",
    "            val_acc_mean, val_acc_std = acc_stat\n",
    "            val_f1_mean,  val_f1_std  = f1_stat\n",
    "\n",
    "            # ============================================================\n",
    "            # [ADD] push to history for plotting\n",
    "            # ============================================================\n",
    "            self.history[\"round\"].append(int(rnd))\n",
    "            self.history[\"p_rate\"].append(float(p_rate_rnd))\n",
    "            self.history[\"val_acc_mean\"].append(float(val_acc_mean))\n",
    "            self.history[\"val_acc_std\"].append(float(val_acc_std))\n",
    "            self.history[\"val_f1_mean\"].append(float(val_f1_mean))\n",
    "            self.history[\"val_f1_std\"].append(float(val_f1_std))\n",
    "            self.history[\"hyper_loss\"].append(float(hyper_loss_total.item() / max(1, len(self.clients))))\n",
    "            self.history[\"inner_loss_mean\"].append(float(inner_loss_mean))\n",
    "            self.history[\"outer_loss_mean\"].append(float(outer_loss_mean))\n",
    "            self.history[\"inner_acc_mean\"].append(float(inner_acc_mean))\n",
    "            self.history[\"outer_acc_mean\"].append(float(outer_acc_mean))\n",
    "\n",
    "            # ============================================================\n",
    "            # [ADD] print p_rate on the same line\n",
    "            # ============================================================\n",
    "            print(\n",
    "                f\"[ROUND {rnd:03d}] \"\n",
    "                f\"VAL(QUERY) Per-Subject ACC = {val_acc_mean*100:.2f}% Â± {val_acc_std*100:.2f}% | \"\n",
    "                f\"Macro-F1 = {val_f1_mean*100:.2f}% Â± {val_f1_std*100:.2f}% | \"\n",
    "                f\"HyperLoss={(hyper_loss_total.item()/max(1,len(self.clients))):.6f} | \"\n",
    "                f\"p_rate={p_rate_rnd:.3f}\"\n",
    "            )\n",
    "\n",
    "            # âœ… select best by VAL/QUERY\n",
    "            if val_acc_mean > best_val_acc:\n",
    "                best_val_acc = val_acc_mean\n",
    "                best_round = rnd\n",
    "                best_p_rate = p_rate_rnd\n",
    "\n",
    "                torch.save(self.global_model.state_dict(), f\"{save_prefix}_bestVAL_global.pth\")\n",
    "                torch.save(self.hypernet.state_dict(), f\"{save_prefix}_bestVAL_hypernet.pth\")\n",
    "                torch.save(self.embed.state_dict(), f\"{save_prefix}_bestVAL_embed.pth\")\n",
    "\n",
    "                meta = {\n",
    "                    \"best_round\": int(best_round),\n",
    "                    \"best_val_acc\": float(best_val_acc),\n",
    "                    \"best_p_rate\": float(best_p_rate),\n",
    "                    \"ff_dim\": int(self.ff_dim),\n",
    "                    \"prune_ratio\": float(PRUNE_RATIO) if \"PRUNE_RATIO\" in globals() else None\n",
    "                }\n",
    "                import json\n",
    "                with open(f\"{save_prefix}_bestVAL_meta.json\", \"w\") as f:\n",
    "                    json.dump(meta, f, indent=2)\n",
    "\n",
    "                print(f\"ðŸ”¥ NEW BEST (by VAL/QUERY) @ Round {rnd} | Acc={best_val_acc*100:.2f}% | p_rate={best_p_rate:.3f}\")\n",
    "\n",
    "        print(\"\\n================ STAGE-2 BEST SUMMARY ================\")\n",
    "        print(f\"Best VAL(QUERY) Accuracy : {best_val_acc*100:.2f}%\")\n",
    "        print(f\"Best Round               : {best_round}\")\n",
    "        print(f\"Best p_rate              : {best_p_rate}\")\n",
    "        print(\"Saved checkpoints:\")\n",
    "        print(f\" - {save_prefix}_bestVAL_global.pth\")\n",
    "        print(f\" - {save_prefix}_bestVAL_hypernet.pth\")\n",
    "        print(f\" - {save_prefix}_bestVAL_embed.pth\")\n",
    "        print(f\" - {save_prefix}_bestVAL_meta.json\")\n",
    "        print(\"======================================================\")\n",
    "\n",
    "        # optional: save final\n",
    "        torch.save(self.global_model.state_dict(), f\"{save_prefix}_final_global.pth\")\n",
    "        torch.save(self.hypernet.state_dict(), f\"{save_prefix}_final_hypernet.pth\")\n",
    "        torch.save(self.embed.state_dict(), f\"{save_prefix}_final_embed.pth\")\n",
    "        print(\"\\nSaved final:\")\n",
    "        print(f\" - {save_prefix}_final_global.pth\")\n",
    "        print(f\" - {save_prefix}_final_hypernet.pth\")\n",
    "        print(f\" - {save_prefix}_final_embed.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20457ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "def load_pfml_npz_grouped(npz_path):\n",
    "    \"\"\"\n",
    "    Load SAD cache (grouped by subject) for PFML.\n",
    "    Expected keys:\n",
    "      - subjects_X: list of (Ni, C, T)\n",
    "      - subjects_Y: list of (Ni,)\n",
    "      - subjects   : list of subject IDs (optional)\n",
    "    \"\"\"\n",
    "    data = np.load(npz_path, allow_pickle=True)\n",
    "\n",
    "    subjects_X = list(data[\"subjects_X\"])\n",
    "    subjects_Y = list(data[\"subjects_Y\"])\n",
    "\n",
    "    if \"subjects\" in data.files:\n",
    "        subject_ids = list(data[\"subjects\"])\n",
    "    else:\n",
    "        subject_ids = list(range(len(subjects_X)))\n",
    "\n",
    "    # sanity\n",
    "    assert len(subjects_X) == len(subjects_Y), \"X/Y subject count mismatch\"\n",
    "\n",
    "    total_samples = sum(len(x) for x in subjects_X)\n",
    "    print(f\"[LOADER] Subjects={len(subjects_X)} | Total samples={total_samples}\")\n",
    "\n",
    "    return subjects_X, subjects_Y, subject_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6399a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CACHE] Loading E:\\Duc\\Safety Driving (2)\\Safety Driving\\SAD\\SAD_cache_paper_hp1.0_lp50.0_fs128_t-3.0_0.0_aug1x1_minAfterAug0.npz\n",
      "[LOADER] Subjects=12 | Total samples=14620\n",
      "[SANITY] subjects: 12\n",
      "[SANITY] X0 shape: (882, 33, 385) Y0 shape: (882,)\n",
      "\n",
      "Total clients(subjects): 12\n",
      "[Class Weight TEMPERED] class0=1.275, class1=0.850\n",
      "[Load] Global backbone: PFML_bestVAL_global.pth\n",
      "[Load] Hypernet: PFML_bestVAL_hypernet.pth\n",
      "[Load] Embed: PFML_bestVAL_embed.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user00\\AppData\\Local\\Temp\\ipykernel_69196\\2816829072.py:52: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_full.load_state_dict(torch.load(CKPT_GLOBAL, map_location=DEVICE), strict=True)\n",
      "C:\\Users\\user00\\AppData\\Local\\Temp\\ipykernel_69196\\2816829072.py:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  hyper_state = torch.load(CKPT_HYPER, map_location=DEVICE) if os.path.exists(CKPT_HYPER) else None\n",
      "C:\\Users\\user00\\AppData\\Local\\Temp\\ipykernel_69196\\2816829072.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embed_state = torch.load(CKPT_EMBED, map_location=DEVICE) if os.path.exists(CKPT_EMBED) else None\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 8) MAIN (script-style, no def main)\n",
    "# ============================================================\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# ============================================================\n",
    "# 1) Load cache\n",
    "# ============================================================\n",
    "if SAVE_NPZ_CACHE and os.path.exists(CACHE_PATH):\n",
    "    print(f\"[CACHE] Loading {CACHE_PATH}\")\n",
    "    subjects_X, subjects_Y, subject_ids = load_pfml_npz_grouped(CACHE_PATH)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Missing cache: {CACHE_PATH}\")\n",
    "\n",
    "print(\"[SANITY] subjects:\", len(subjects_X))\n",
    "print(\"[SANITY] X0 shape:\", subjects_X[0].shape, \"Y0 shape:\", subjects_Y[0].shape)\n",
    "\n",
    "num_client = len(subjects_X)\n",
    "print(f\"\\nTotal clients(subjects): {num_client}\")\n",
    "if num_client < 2:\n",
    "    raise RuntimeError(\"Need at least 2 subjects/clients.\")\n",
    "\n",
    "# ============================================================\n",
    "# 2) Class weights\n",
    "# ============================================================\n",
    "class_weights = compute_class_weights(subjects_Y).to(DEVICE)\n",
    "\n",
    "# ============================================================\n",
    "# 3) Paper-style split: Train/Test â†’ Support/Query\n",
    "# ============================================================\n",
    "TRAIN_RATE = 0.7\n",
    "sup_sets, que_sets, test_sets = make_train_support_query_test_sets(\n",
    "    subjects_X, subjects_Y,\n",
    "    train_rate=TRAIN_RATE,\n",
    "    qr_split_rate=QR_SPLIT_RATE\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 4) Build full model + load Stage-1 checkpoints\n",
    "# ============================================================\n",
    "ff_full = 2 * D_MODEL\n",
    "model_full = EEGTransformerPrunable(\n",
    "    C=SAD_C, T=SAD_T,\n",
    "    d_model=D_MODEL, nhead=N_HEAD,\n",
    "    num_layers=N_LAYERS,\n",
    "    ff_dim=ff_full,\n",
    "    num_classes=2\n",
    ").to(DEVICE)\n",
    "\n",
    "if os.path.exists(CKPT_GLOBAL):\n",
    "    model_full.load_state_dict(torch.load(CKPT_GLOBAL, map_location=DEVICE), strict=True)\n",
    "    print(f\"[Load] Global backbone: {CKPT_GLOBAL}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Missing {CKPT_GLOBAL} in current folder.\")\n",
    "\n",
    "hyper_state = torch.load(CKPT_HYPER, map_location=DEVICE) if os.path.exists(CKPT_HYPER) else None\n",
    "embed_state = torch.load(CKPT_EMBED, map_location=DEVICE) if os.path.exists(CKPT_EMBED) else None\n",
    "if hyper_state is not None:\n",
    "    print(f\"[Load] Hypernet: {CKPT_HYPER}\")\n",
    "if embed_state is not None:\n",
    "    print(f\"[Load] Embed: {CKPT_EMBED}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f46ee8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== BEFORE PRUNING ==========\n",
      "[Params] total=173,099 | trainable=173,099 | ff_dim=128\n",
      "===================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 5) Params BEFORE pruning\n",
    "# ============================================================\n",
    "total_b, train_b = count_params(model_full)\n",
    "print(\"\\n========== BEFORE PRUNING ==========\")\n",
    "print(f\"[Params] total={total_b:,} | trainable={train_b:,} | ff_dim={ff_full}\")\n",
    "print(\"===================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d461bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== AFTER PRUNING (INIT) ==========\n",
      "[Params] total=131,819 | trainable=131,819 | ff_dim=64\n",
      "[Keep]  76.15% params remain\n",
      "=========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 6) Prune FFN neurons (ICÂ²â€“ASTER)\n",
    "# ============================================================\n",
    "model_pruned = prune_ffn_neurons(\n",
    "    model_full,\n",
    "    prune_ratio=PRUNE_RATIO,\n",
    "    sigma=SIGMA,\n",
    "    alpha=ALPHA,\n",
    "    k_nn=K_NN,\n",
    "    n_bins=N_BINS\n",
    ")\n",
    "\n",
    "total_a, train_a = count_params(model_pruned)\n",
    "print(\"\\n========== AFTER PRUNING (INIT) ==========\")\n",
    "print(f\"[Params] total={total_a:,} | trainable={train_a:,} | ff_dim={model_pruned.ff_dim}\")\n",
    "print(f\"[Keep]  {100 * total_a / total_b:.2f}% params remain\")\n",
    "print(\"=========================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50d527cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-1 META] best_p_rate=0.591 (will be used as Stage-2 cap)\n"
     ]
    }
   ],
   "source": [
    "# ---- NEW: load best_p_rate from Stage-1 meta (if exists) ----\n",
    "p_rate_cap = P_RATE  # default\n",
    "if os.path.exists(CKPT_META):\n",
    "    import json\n",
    "    with open(CKPT_META, \"r\") as f:\n",
    "        meta1 = json.load(f)\n",
    "    p_rate_cap = float(meta1.get(\"best_p_rate\", P_RATE))\n",
    "    print(f\"[Stage-1 META] best_p_rate={p_rate_cap:.3f} (will be used as Stage-2 cap)\")\n",
    "else:\n",
    "    print(f\"[Stage-1 META] not found: {CKPT_META} -> use default cap P_RATE={P_RATE:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ae692be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PFML] Created 12 clients.\n",
      "[PFML Stage-2] Start training on cpu, rounds=100, ff_dim=64\n",
      "\n",
      "================ PFML INTRA (VAL@Round001) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 93.83%  (Std 3.37%)\n",
      "Mean Macro-F1 : 92.21%  (Std 4.32%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 98.79% | Macro-F1= 98.11%\n",
      "  cid= 1 | N=  529 | ACC= 96.22% | Macro-F1= 94.58%\n",
      "  cid= 2 | N=  321 | ACC= 88.16% | Macro-F1= 86.05%\n",
      "  cid= 3 | N=  182 | ACC= 91.76% | Macro-F1= 88.09%\n",
      "  cid= 4 | N=  222 | ACC= 92.79% | Macro-F1= 91.38%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 92.70% | Macro-F1= 92.64%\n",
      "  cid= 7 | N=  113 | ACC= 94.69% | Macro-F1= 94.66%\n",
      "  cid= 8 | N=  476 | ACC= 90.13% | Macro-F1= 86.65%\n",
      "  cid= 9 | N=  780 | ACC= 92.18% | Macro-F1= 91.21%\n",
      "  cid=10 | N=  254 | ACC= 94.49% | Macro-F1= 89.40%\n",
      "  cid=11 | N=  254 | ACC= 94.09% | Macro-F1= 93.76%\n",
      "[ROUND 001] VAL(QUERY) Per-Subject ACC = 93.83% Â± 3.37% | Macro-F1 = 92.21% Â± 4.32% | HyperLoss=0.000019 | p_rate=0.591\n",
      "ðŸ”¥ NEW BEST (by VAL/QUERY) @ Round 1 | Acc=93.83% | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round002) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 94.67%  (Std 3.36%)\n",
      "Mean Macro-F1 : 93.25%  (Std 4.32%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 96.22% | Macro-F1= 94.55%\n",
      "  cid= 2 | N=  321 | ACC= 89.10% | Macro-F1= 87.18%\n",
      "  cid= 3 | N=  182 | ACC= 90.11% | Macro-F1= 85.83%\n",
      "  cid= 4 | N=  222 | ACC= 94.14% | Macro-F1= 92.90%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 92.17% | Macro-F1= 92.09%\n",
      "  cid= 7 | N=  113 | ACC= 96.46% | Macro-F1= 96.43%\n",
      "  cid= 8 | N=  476 | ACC= 93.70% | Macro-F1= 91.19%\n",
      "  cid= 9 | N=  780 | ACC= 93.59% | Macro-F1= 92.72%\n",
      "  cid=10 | N=  254 | ACC= 95.67% | Macro-F1= 91.58%\n",
      "  cid=11 | N=  254 | ACC= 94.88% | Macro-F1= 94.59%\n",
      "[ROUND 002] VAL(QUERY) Per-Subject ACC = 94.67% Â± 3.36% | Macro-F1 = 93.25% Â± 4.32% | HyperLoss=0.000224 | p_rate=0.591\n",
      "ðŸ”¥ NEW BEST (by VAL/QUERY) @ Round 2 | Acc=94.67% | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round003) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 95.56%  (Std 3.60%)\n",
      "Mean Macro-F1 : 94.42%  (Std 4.24%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 96.41% | Macro-F1= 94.83%\n",
      "  cid= 2 | N=  321 | ACC= 87.85% | Macro-F1= 86.03%\n",
      "  cid= 3 | N=  182 | ACC= 92.86% | Macro-F1= 89.29%\n",
      "  cid= 4 | N=  222 | ACC= 96.85% | Macro-F1= 96.06%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 91.30% | Macro-F1= 91.19%\n",
      "  cid= 7 | N=  113 | ACC= 98.23% | Macro-F1= 98.22%\n",
      "  cid= 8 | N=  476 | ACC= 94.33% | Macro-F1= 91.78%\n",
      "  cid= 9 | N=  780 | ACC= 94.74% | Macro-F1= 93.92%\n",
      "  cid=10 | N=  254 | ACC= 98.03% | Macro-F1= 95.91%\n",
      "  cid=11 | N=  254 | ACC= 96.06% | Macro-F1= 95.81%\n",
      "[ROUND 003] VAL(QUERY) Per-Subject ACC = 95.56% Â± 3.60% | Macro-F1 = 94.42% Â± 4.24% | HyperLoss=0.000028 | p_rate=0.591\n",
      "ðŸ”¥ NEW BEST (by VAL/QUERY) @ Round 3 | Acc=95.56% | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round004) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 95.18%  (Std 3.13%)\n",
      "Mean Macro-F1 : 93.79%  (Std 4.11%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 96.60% | Macro-F1= 95.09%\n",
      "  cid= 2 | N=  321 | ACC= 89.72% | Macro-F1= 87.98%\n",
      "  cid= 3 | N=  182 | ACC= 92.31% | Macro-F1= 88.79%\n",
      "  cid= 4 | N=  222 | ACC= 94.14% | Macro-F1= 92.90%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 92.87% | Macro-F1= 92.80%\n",
      "  cid= 7 | N=  113 | ACC= 98.23% | Macro-F1= 98.21%\n",
      "  cid= 8 | N=  476 | ACC= 93.49% | Macro-F1= 90.75%\n",
      "  cid= 9 | N=  780 | ACC= 93.85% | Macro-F1= 92.95%\n",
      "  cid=10 | N=  254 | ACC= 94.88% | Macro-F1= 90.26%\n",
      "  cid=11 | N=  254 | ACC= 96.06% | Macro-F1= 95.81%\n",
      "[ROUND 004] VAL(QUERY) Per-Subject ACC = 95.18% Â± 3.13% | Macro-F1 = 93.79% Â± 4.11% | HyperLoss=0.000051 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round005) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.02%  (Std 2.96%)\n",
      "Mean Macro-F1 : 94.86%  (Std 3.69%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 96.98% | Macro-F1= 95.61%\n",
      "  cid= 2 | N=  321 | ACC= 89.72% | Macro-F1= 88.24%\n",
      "  cid= 3 | N=  182 | ACC= 93.96% | Macro-F1= 90.76%\n",
      "  cid= 4 | N=  222 | ACC= 97.75% | Macro-F1= 97.16%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.22% | Macro-F1= 93.12%\n",
      "  cid= 7 | N=  113 | ACC= 98.23% | Macro-F1= 98.22%\n",
      "  cid= 8 | N=  476 | ACC= 94.96% | Macro-F1= 92.67%\n",
      "  cid= 9 | N=  780 | ACC= 94.49% | Macro-F1= 93.64%\n",
      "  cid=10 | N=  254 | ACC= 96.06% | Macro-F1= 92.26%\n",
      "  cid=11 | N=  254 | ACC= 96.85% | Macro-F1= 96.64%\n",
      "[ROUND 005] VAL(QUERY) Per-Subject ACC = 96.02% Â± 2.96% | Macro-F1 = 94.86% Â± 3.69% | HyperLoss=0.000020 | p_rate=0.591\n",
      "ðŸ”¥ NEW BEST (by VAL/QUERY) @ Round 5 | Acc=96.02% | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round006) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.07%  (Std 3.08%)\n",
      "Mean Macro-F1 : 95.02%  (Std 3.61%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.35% | Macro-F1= 96.18%\n",
      "  cid= 2 | N=  321 | ACC= 89.72% | Macro-F1= 88.05%\n",
      "  cid= 3 | N=  182 | ACC= 95.05% | Macro-F1= 92.29%\n",
      "  cid= 4 | N=  222 | ACC= 96.85% | Macro-F1= 96.10%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 92.35% | Macro-F1= 92.24%\n",
      "  cid= 7 | N=  113 | ACC= 98.23% | Macro-F1= 98.22%\n",
      "  cid= 8 | N=  476 | ACC= 94.54% | Macro-F1= 92.06%\n",
      "  cid= 9 | N=  780 | ACC= 93.85% | Macro-F1= 92.94%\n",
      "  cid=10 | N=  254 | ACC= 97.64% | Macro-F1= 95.15%\n",
      "  cid=11 | N=  254 | ACC= 97.24% | Macro-F1= 97.05%\n",
      "[ROUND 006] VAL(QUERY) Per-Subject ACC = 96.07% Â± 3.08% | Macro-F1 = 95.02% Â± 3.61% | HyperLoss=0.000011 | p_rate=0.591\n",
      "ðŸ”¥ NEW BEST (by VAL/QUERY) @ Round 6 | Acc=96.07% | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round007) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.30%  (Std 3.01%)\n",
      "Mean Macro-F1 : 95.27%  (Std 3.60%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.35% | Macro-F1= 96.18%\n",
      "  cid= 2 | N=  321 | ACC= 90.03% | Macro-F1= 88.32%\n",
      "  cid= 3 | N=  182 | ACC= 95.60% | Macro-F1= 93.07%\n",
      "  cid= 4 | N=  222 | ACC= 97.30% | Macro-F1= 96.61%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.22% | Macro-F1= 93.14%\n",
      "  cid= 7 | N=  113 | ACC= 99.12% | Macro-F1= 99.11%\n",
      "  cid= 8 | N=  476 | ACC= 93.91% | Macro-F1= 91.23%\n",
      "  cid= 9 | N=  780 | ACC= 94.23% | Macro-F1= 93.38%\n",
      "  cid=10 | N=  254 | ACC= 97.64% | Macro-F1= 95.15%\n",
      "  cid=11 | N=  254 | ACC= 97.24% | Macro-F1= 97.05%\n",
      "[ROUND 007] VAL(QUERY) Per-Subject ACC = 96.30% Â± 3.01% | Macro-F1 = 95.27% Â± 3.60% | HyperLoss=0.000008 | p_rate=0.591\n",
      "ðŸ”¥ NEW BEST (by VAL/QUERY) @ Round 7 | Acc=96.30% | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round008) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.31%  (Std 3.61%)\n",
      "Mean Macro-F1 : 95.44%  (Std 4.02%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.16% | Macro-F1= 95.90%\n",
      "  cid= 2 | N=  321 | ACC= 87.85% | Macro-F1= 86.36%\n",
      "  cid= 3 | N=  182 | ACC= 96.70% | Macro-F1= 94.81%\n",
      "  cid= 4 | N=  222 | ACC= 97.30% | Macro-F1= 96.54%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 91.65% | Macro-F1= 91.52%\n",
      "  cid= 7 | N=  113 | ACC= 99.12% | Macro-F1= 99.11%\n",
      "  cid= 8 | N=  476 | ACC= 94.33% | Macro-F1= 91.78%\n",
      "  cid= 9 | N=  780 | ACC= 95.13% | Macro-F1= 94.32%\n",
      "  cid=10 | N=  254 | ACC= 98.82% | Macro-F1= 97.48%\n",
      "  cid=11 | N=  254 | ACC= 97.64% | Macro-F1= 97.47%\n",
      "[ROUND 008] VAL(QUERY) Per-Subject ACC = 96.31% Â± 3.61% | Macro-F1 = 95.44% Â± 4.02% | HyperLoss=0.000007 | p_rate=0.591\n",
      "ðŸ”¥ NEW BEST (by VAL/QUERY) @ Round 8 | Acc=96.31% | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round009) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.29%  (Std 2.98%)\n",
      "Mean Macro-F1 : 95.27%  (Std 3.63%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 96.98% | Macro-F1= 95.64%\n",
      "  cid= 2 | N=  321 | ACC= 90.34% | Macro-F1= 88.83%\n",
      "  cid= 3 | N=  182 | ACC= 93.96% | Macro-F1= 90.76%\n",
      "  cid= 4 | N=  222 | ACC= 97.75% | Macro-F1= 97.16%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.04% | Macro-F1= 92.95%\n",
      "  cid= 7 | N=  113 | ACC= 99.12% | Macro-F1= 99.11%\n",
      "  cid= 8 | N=  476 | ACC= 94.33% | Macro-F1= 91.84%\n",
      "  cid= 9 | N=  780 | ACC= 95.13% | Macro-F1= 94.37%\n",
      "  cid=10 | N=  254 | ACC= 98.03% | Macro-F1= 95.91%\n",
      "  cid=11 | N=  254 | ACC= 96.85% | Macro-F1= 96.64%\n",
      "[ROUND 009] VAL(QUERY) Per-Subject ACC = 96.29% Â± 2.98% | Macro-F1 = 95.27% Â± 3.63% | HyperLoss=0.000006 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round010) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.21%  (Std 3.54%)\n",
      "Mean Macro-F1 : 95.25%  (Std 3.99%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 96.79% | Macro-F1= 95.23%\n",
      "  cid= 2 | N=  321 | ACC= 88.79% | Macro-F1= 87.32%\n",
      "  cid= 3 | N=  182 | ACC= 95.05% | Macro-F1= 92.29%\n",
      "  cid= 4 | N=  222 | ACC= 96.40% | Macro-F1= 95.38%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 91.13% | Macro-F1= 90.98%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 94.96% | Macro-F1= 92.67%\n",
      "  cid= 9 | N=  780 | ACC= 95.00% | Macro-F1= 94.15%\n",
      "  cid=10 | N=  254 | ACC= 98.82% | Macro-F1= 97.48%\n",
      "  cid=11 | N=  254 | ACC= 97.64% | Macro-F1= 97.47%\n",
      "[ROUND 010] VAL(QUERY) Per-Subject ACC = 96.21% Â± 3.54% | Macro-F1 = 95.25% Â± 3.99% | HyperLoss=0.000006 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round011) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.67%  (Std 2.89%)\n",
      "Mean Macro-F1 : 95.71%  (Std 3.43%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.35% | Macro-F1= 96.16%\n",
      "  cid= 2 | N=  321 | ACC= 90.65% | Macro-F1= 89.33%\n",
      "  cid= 3 | N=  182 | ACC= 96.15% | Macro-F1= 94.00%\n",
      "  cid= 4 | N=  222 | ACC= 98.20% | Macro-F1= 97.72%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.57% | Macro-F1= 93.50%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 94.54% | Macro-F1= 92.06%\n",
      "  cid= 9 | N=  780 | ACC= 94.74% | Macro-F1= 93.91%\n",
      "  cid=10 | N=  254 | ACC= 97.24% | Macro-F1= 94.40%\n",
      "  cid=11 | N=  254 | ACC= 97.64% | Macro-F1= 97.47%\n",
      "[ROUND 011] VAL(QUERY) Per-Subject ACC = 96.67% Â± 2.89% | Macro-F1 = 95.71% Â± 3.43% | HyperLoss=0.000009 | p_rate=0.591\n",
      "ðŸ”¥ NEW BEST (by VAL/QUERY) @ Round 11 | Acc=96.67% | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round012) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.79%  (Std 3.37%)\n",
      "Mean Macro-F1 : 95.98%  (Std 3.73%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.54% | Macro-F1= 96.42%\n",
      "  cid= 2 | N=  321 | ACC= 89.10% | Macro-F1= 87.59%\n",
      "  cid= 3 | N=  182 | ACC= 96.70% | Macro-F1= 94.69%\n",
      "  cid= 4 | N=  222 | ACC= 96.85% | Macro-F1= 95.98%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 91.83% | Macro-F1= 91.67%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 96.22% | Macro-F1= 94.34%\n",
      "  cid= 9 | N=  780 | ACC= 95.64% | Macro-F1= 94.88%\n",
      "  cid=10 | N=  254 | ACC= 98.82% | Macro-F1= 97.48%\n",
      "  cid=11 | N=  254 | ACC= 98.82% | Macro-F1= 98.72%\n",
      "[ROUND 012] VAL(QUERY) Per-Subject ACC = 96.79% Â± 3.37% | Macro-F1 = 95.98% Â± 3.73% | HyperLoss=0.000007 | p_rate=0.591\n",
      "ðŸ”¥ NEW BEST (by VAL/QUERY) @ Round 12 | Acc=96.79% | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round013) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.57%  (Std 2.82%)\n",
      "Mean Macro-F1 : 95.59%  (Std 3.49%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.54% | Macro-F1= 96.44%\n",
      "  cid= 2 | N=  321 | ACC= 91.28% | Macro-F1= 89.94%\n",
      "  cid= 3 | N=  182 | ACC= 96.15% | Macro-F1= 94.00%\n",
      "  cid= 4 | N=  222 | ACC= 98.65% | Macro-F1= 98.30%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 94.61% | Macro-F1= 94.56%\n",
      "  cid= 7 | N=  113 | ACC= 99.12% | Macro-F1= 99.11%\n",
      "  cid= 8 | N=  476 | ACC= 93.07% | Macro-F1= 90.15%\n",
      "  cid= 9 | N=  780 | ACC= 93.97% | Macro-F1= 93.09%\n",
      "  cid=10 | N=  254 | ACC= 97.24% | Macro-F1= 94.40%\n",
      "  cid=11 | N=  254 | ACC= 97.24% | Macro-F1= 97.05%\n",
      "[ROUND 013] VAL(QUERY) Per-Subject ACC = 96.57% Â± 2.82% | Macro-F1 = 95.59% Â± 3.49% | HyperLoss=0.000007 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round014) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.32%  (Std 3.30%)\n",
      "Mean Macro-F1 : 95.26%  (Std 3.76%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 97.73% | Macro-F1= 96.69%\n",
      "  cid= 2 | N=  321 | ACC= 89.10% | Macro-F1= 87.70%\n",
      "  cid= 3 | N=  182 | ACC= 95.60% | Macro-F1= 93.07%\n",
      "  cid= 4 | N=  222 | ACC= 96.40% | Macro-F1= 95.28%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 92.17% | Macro-F1= 92.04%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 94.96% | Macro-F1= 92.62%\n",
      "  cid= 9 | N=  780 | ACC= 94.62% | Macro-F1= 93.70%\n",
      "  cid=10 | N=  254 | ACC= 97.24% | Macro-F1= 94.40%\n",
      "  cid=11 | N=  254 | ACC= 98.43% | Macro-F1= 98.30%\n",
      "[ROUND 014] VAL(QUERY) Per-Subject ACC = 96.32% Â± 3.30% | Macro-F1 = 95.26% Â± 3.76% | HyperLoss=0.000006 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round015) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.05%  (Std 3.34%)\n",
      "Mean Macro-F1 : 94.88%  (Std 4.21%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 97.35% | Macro-F1= 96.16%\n",
      "  cid= 2 | N=  321 | ACC= 89.10% | Macro-F1= 87.18%\n",
      "  cid= 3 | N=  182 | ACC= 92.31% | Macro-F1= 88.79%\n",
      "  cid= 4 | N=  222 | ACC= 97.75% | Macro-F1= 97.13%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.91% | Macro-F1= 93.84%\n",
      "  cid= 7 | N=  113 | ACC= 99.12% | Macro-F1= 99.11%\n",
      "  cid= 8 | N=  476 | ACC= 93.49% | Macro-F1= 90.81%\n",
      "  cid= 9 | N=  780 | ACC= 94.74% | Macro-F1= 93.96%\n",
      "  cid=10 | N=  254 | ACC= 97.24% | Macro-F1= 94.40%\n",
      "  cid=11 | N=  254 | ACC= 98.03% | Macro-F1= 97.88%\n",
      "[ROUND 015] VAL(QUERY) Per-Subject ACC = 96.05% Â± 3.34% | Macro-F1 = 94.88% Â± 4.21% | HyperLoss=0.000005 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round016) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.65%  (Std 3.08%)\n",
      "Mean Macro-F1 : 95.77%  (Std 3.53%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.35% | Macro-F1= 96.16%\n",
      "  cid= 2 | N=  321 | ACC= 89.72% | Macro-F1= 88.24%\n",
      "  cid= 3 | N=  182 | ACC= 96.70% | Macro-F1= 94.81%\n",
      "  cid= 4 | N=  222 | ACC= 97.30% | Macro-F1= 96.57%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 92.87% | Macro-F1= 92.76%\n",
      "  cid= 7 | N=  113 | ACC= 99.12% | Macro-F1= 99.11%\n",
      "  cid= 8 | N=  476 | ACC= 94.54% | Macro-F1= 92.00%\n",
      "  cid= 9 | N=  780 | ACC= 95.38% | Macro-F1= 94.64%\n",
      "  cid=10 | N=  254 | ACC= 98.43% | Macro-F1= 96.69%\n",
      "  cid=11 | N=  254 | ACC= 98.43% | Macro-F1= 98.30%\n",
      "[ROUND 016] VAL(QUERY) Per-Subject ACC = 96.65% Â± 3.08% | Macro-F1 = 95.77% Â± 3.53% | HyperLoss=0.000006 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round017) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.33%  (Std 2.91%)\n",
      "Mean Macro-F1 : 95.29%  (Std 3.58%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 97.54% | Macro-F1= 96.42%\n",
      "  cid= 2 | N=  321 | ACC= 90.97% | Macro-F1= 89.55%\n",
      "  cid= 3 | N=  182 | ACC= 93.41% | Macro-F1= 90.21%\n",
      "  cid= 4 | N=  222 | ACC= 96.40% | Macro-F1= 95.43%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.39% | Macro-F1= 93.31%\n",
      "  cid= 7 | N=  113 | ACC= 99.12% | Macro-F1= 99.11%\n",
      "  cid= 8 | N=  476 | ACC= 94.12% | Macro-F1= 91.62%\n",
      "  cid= 9 | N=  780 | ACC= 95.00% | Macro-F1= 94.23%\n",
      "  cid=10 | N=  254 | ACC= 98.03% | Macro-F1= 95.91%\n",
      "  cid=11 | N=  254 | ACC= 98.43% | Macro-F1= 98.30%\n",
      "[ROUND 017] VAL(QUERY) Per-Subject ACC = 96.33% Â± 2.91% | Macro-F1 = 95.29% Â± 3.58% | HyperLoss=0.000008 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round018) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.86%  (Std 2.72%)\n",
      "Mean Macro-F1 : 96.00%  (Std 3.11%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.16% | Macro-F1= 95.87%\n",
      "  cid= 2 | N=  321 | ACC= 90.97% | Macro-F1= 89.55%\n",
      "  cid= 3 | N=  182 | ACC= 96.70% | Macro-F1= 94.81%\n",
      "  cid= 4 | N=  222 | ACC= 97.75% | Macro-F1= 97.13%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.39% | Macro-F1= 93.30%\n",
      "  cid= 7 | N=  113 | ACC= 99.12% | Macro-F1= 99.11%\n",
      "  cid= 8 | N=  476 | ACC= 96.01% | Macro-F1= 94.05%\n",
      "  cid= 9 | N=  780 | ACC= 94.74% | Macro-F1= 93.93%\n",
      "  cid=10 | N=  254 | ACC= 98.03% | Macro-F1= 95.91%\n",
      "  cid=11 | N=  254 | ACC= 98.43% | Macro-F1= 98.30%\n",
      "[ROUND 018] VAL(QUERY) Per-Subject ACC = 96.86% Â± 2.72% | Macro-F1 = 96.00% Â± 3.11% | HyperLoss=0.000007 | p_rate=0.591\n",
      "ðŸ”¥ NEW BEST (by VAL/QUERY) @ Round 18 | Acc=96.86% | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round019) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.74%  (Std 2.85%)\n",
      "Mean Macro-F1 : 95.89%  (Std 3.29%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.54% | Macro-F1= 96.44%\n",
      "  cid= 2 | N=  321 | ACC= 90.34% | Macro-F1= 88.77%\n",
      "  cid= 3 | N=  182 | ACC= 97.25% | Macro-F1= 95.72%\n",
      "  cid= 4 | N=  222 | ACC= 97.75% | Macro-F1= 97.13%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.74% | Macro-F1= 93.67%\n",
      "  cid= 7 | N=  113 | ACC= 99.12% | Macro-F1= 99.11%\n",
      "  cid= 8 | N=  476 | ACC= 94.96% | Macro-F1= 92.77%\n",
      "  cid= 9 | N=  780 | ACC= 94.49% | Macro-F1= 93.66%\n",
      "  cid=10 | N=  254 | ACC= 98.03% | Macro-F1= 95.91%\n",
      "  cid=11 | N=  254 | ACC= 97.64% | Macro-F1= 97.47%\n",
      "[ROUND 019] VAL(QUERY) Per-Subject ACC = 96.74% Â± 2.85% | Macro-F1 = 95.89% Â± 3.29% | HyperLoss=0.000009 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round020) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.56%  (Std 2.97%)\n",
      "Mean Macro-F1 : 95.61%  (Std 3.40%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 96.79% | Macro-F1= 95.38%\n",
      "  cid= 2 | N=  321 | ACC= 90.34% | Macro-F1= 88.83%\n",
      "  cid= 3 | N=  182 | ACC= 97.25% | Macro-F1= 95.63%\n",
      "  cid= 4 | N=  222 | ACC= 95.95% | Macro-F1= 94.78%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.04% | Macro-F1= 92.98%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 95.17% | Macro-F1= 93.04%\n",
      "  cid= 9 | N=  780 | ACC= 94.49% | Macro-F1= 93.62%\n",
      "  cid=10 | N=  254 | ACC= 97.64% | Macro-F1= 95.15%\n",
      "  cid=11 | N=  254 | ACC= 98.03% | Macro-F1= 97.88%\n",
      "[ROUND 020] VAL(QUERY) Per-Subject ACC = 96.56% Â± 2.97% | Macro-F1 = 95.61% Â± 3.40% | HyperLoss=0.000006 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round021) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.68%  (Std 3.30%)\n",
      "Mean Macro-F1 : 95.81%  (Std 3.57%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 97.16% | Macro-F1= 95.82%\n",
      "  cid= 2 | N=  321 | ACC= 89.41% | Macro-F1= 87.97%\n",
      "  cid= 3 | N=  182 | ACC= 97.25% | Macro-F1= 95.63%\n",
      "  cid= 4 | N=  222 | ACC= 97.75% | Macro-F1= 97.13%\n",
      "  cid= 5 | N=  141 | ACC= 99.29% | Macro-F1= 99.18%\n",
      "  cid= 6 | N=  575 | ACC= 91.13% | Macro-F1= 90.92%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 96.22% | Macro-F1= 94.34%\n",
      "  cid= 9 | N=  780 | ACC= 95.51% | Macro-F1= 94.76%\n",
      "  cid=10 | N=  254 | ACC= 98.03% | Macro-F1= 95.91%\n",
      "  cid=11 | N=  254 | ACC= 98.82% | Macro-F1= 98.72%\n",
      "[ROUND 021] VAL(QUERY) Per-Subject ACC = 96.68% Â± 3.30% | Macro-F1 = 95.81% Â± 3.57% | HyperLoss=0.000006 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round022) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.57%  (Std 3.07%)\n",
      "Mean Macro-F1 : 95.63%  (Std 3.59%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.35% | Macro-F1= 96.18%\n",
      "  cid= 2 | N=  321 | ACC= 89.72% | Macro-F1= 88.05%\n",
      "  cid= 3 | N=  182 | ACC= 96.70% | Macro-F1= 94.91%\n",
      "  cid= 4 | N=  222 | ACC= 96.40% | Macro-F1= 95.33%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 94.09% | Macro-F1= 94.02%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 94.33% | Macro-F1= 91.94%\n",
      "  cid= 9 | N=  780 | ACC= 94.23% | Macro-F1= 93.38%\n",
      "  cid=10 | N=  254 | ACC= 98.03% | Macro-F1= 95.91%\n",
      "  cid=11 | N=  254 | ACC= 98.03% | Macro-F1= 97.88%\n",
      "[ROUND 022] VAL(QUERY) Per-Subject ACC = 96.57% Â± 3.07% | Macro-F1 = 95.63% Â± 3.59% | HyperLoss=0.000006 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round023) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.69%  (Std 3.54%)\n",
      "Mean Macro-F1 : 95.87%  (Std 3.85%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 97.92% | Macro-F1= 96.95%\n",
      "  cid= 2 | N=  321 | ACC= 88.16% | Macro-F1= 86.68%\n",
      "  cid= 3 | N=  182 | ACC= 96.70% | Macro-F1= 94.69%\n",
      "  cid= 4 | N=  222 | ACC= 96.85% | Macro-F1= 95.89%\n",
      "  cid= 5 | N=  141 | ACC= 99.29% | Macro-F1= 99.18%\n",
      "  cid= 6 | N=  575 | ACC= 91.65% | Macro-F1= 91.45%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 96.22% | Macro-F1= 94.39%\n",
      "  cid= 9 | N=  780 | ACC= 95.90% | Macro-F1= 95.17%\n",
      "  cid=10 | N=  254 | ACC= 98.82% | Macro-F1= 97.48%\n",
      "  cid=11 | N=  254 | ACC= 99.21% | Macro-F1= 99.15%\n",
      "[ROUND 023] VAL(QUERY) Per-Subject ACC = 96.69% Â± 3.54% | Macro-F1 = 95.87% Â± 3.85% | HyperLoss=0.000005 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round024) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.40%  (Std 2.93%)\n",
      "Mean Macro-F1 : 95.32%  (Std 3.66%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.35% | Macro-F1= 96.21%\n",
      "  cid= 2 | N=  321 | ACC= 90.03% | Macro-F1= 88.44%\n",
      "  cid= 3 | N=  182 | ACC= 94.51% | Macro-F1= 91.84%\n",
      "  cid= 4 | N=  222 | ACC= 97.30% | Macro-F1= 96.57%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.91% | Macro-F1= 93.84%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 95.17% | Macro-F1= 93.09%\n",
      "  cid= 9 | N=  780 | ACC= 95.26% | Macro-F1= 94.54%\n",
      "  cid=10 | N=  254 | ACC= 96.06% | Macro-F1= 92.26%\n",
      "  cid=11 | N=  254 | ACC= 97.24% | Macro-F1= 97.05%\n",
      "[ROUND 024] VAL(QUERY) Per-Subject ACC = 96.40% Â± 2.93% | Macro-F1 = 95.32% Â± 3.66% | HyperLoss=0.000005 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round025) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.67%  (Std 3.26%)\n",
      "Mean Macro-F1 : 95.79%  (Std 3.66%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 96.22% | Macro-F1= 94.48%\n",
      "  cid= 2 | N=  321 | ACC= 89.10% | Macro-F1= 87.52%\n",
      "  cid= 3 | N=  182 | ACC= 97.80% | Macro-F1= 96.54%\n",
      "  cid= 4 | N=  222 | ACC= 97.30% | Macro-F1= 96.54%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 92.35% | Macro-F1= 92.20%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 95.59% | Macro-F1= 93.47%\n",
      "  cid= 9 | N=  780 | ACC= 95.64% | Macro-F1= 94.94%\n",
      "  cid=10 | N=  254 | ACC= 98.03% | Macro-F1= 95.91%\n",
      "  cid=11 | N=  254 | ACC= 98.03% | Macro-F1= 97.88%\n",
      "[ROUND 025] VAL(QUERY) Per-Subject ACC = 96.67% Â± 3.26% | Macro-F1 = 95.79% Â± 3.66% | HyperLoss=0.000007 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round026) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 95.85%  (Std 2.97%)\n",
      "Mean Macro-F1 : 94.62%  (Std 3.92%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 96.98% | Macro-F1= 95.74%\n",
      "  cid= 2 | N=  321 | ACC= 90.65% | Macro-F1= 89.05%\n",
      "  cid= 3 | N=  182 | ACC= 93.41% | Macro-F1= 90.39%\n",
      "  cid= 4 | N=  222 | ACC= 97.75% | Macro-F1= 97.16%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 94.96% | Macro-F1= 94.93%\n",
      "  cid= 7 | N=  113 | ACC= 99.12% | Macro-F1= 99.11%\n",
      "  cid= 8 | N=  476 | ACC= 93.28% | Macro-F1= 90.60%\n",
      "  cid= 9 | N=  780 | ACC= 93.46% | Macro-F1= 92.58%\n",
      "  cid=10 | N=  254 | ACC= 95.28% | Macro-F1= 90.91%\n",
      "  cid=11 | N=  254 | ACC= 95.28% | Macro-F1= 95.00%\n",
      "[ROUND 026] VAL(QUERY) Per-Subject ACC = 95.85% Â± 2.97% | Macro-F1 = 94.62% Â± 3.92% | HyperLoss=0.000007 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round027) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.92%  (Std 3.23%)\n",
      "Mean Macro-F1 : 96.13%  (Std 3.52%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 97.73% | Macro-F1= 96.71%\n",
      "  cid= 2 | N=  321 | ACC= 89.41% | Macro-F1= 87.97%\n",
      "  cid= 3 | N=  182 | ACC= 97.80% | Macro-F1= 96.46%\n",
      "  cid= 4 | N=  222 | ACC= 97.30% | Macro-F1= 96.50%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 92.17% | Macro-F1= 92.01%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 96.22% | Macro-F1= 94.34%\n",
      "  cid= 9 | N=  780 | ACC= 95.51% | Macro-F1= 94.76%\n",
      "  cid=10 | N=  254 | ACC= 98.43% | Macro-F1= 96.69%\n",
      "  cid=11 | N=  254 | ACC= 98.82% | Macro-F1= 98.72%\n",
      "[ROUND 027] VAL(QUERY) Per-Subject ACC = 96.92% Â± 3.23% | Macro-F1 = 96.13% Â± 3.52% | HyperLoss=0.000009 | p_rate=0.591\n",
      "ðŸ”¥ NEW BEST (by VAL/QUERY) @ Round 27 | Acc=96.92% | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round028) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 95.85%  (Std 3.08%)\n",
      "Mean Macro-F1 : 94.62%  (Std 4.04%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.54% | Macro-F1= 96.47%\n",
      "  cid= 2 | N=  321 | ACC= 91.28% | Macro-F1= 89.66%\n",
      "  cid= 3 | N=  182 | ACC= 92.31% | Macro-F1= 88.98%\n",
      "  cid= 4 | N=  222 | ACC= 96.40% | Macro-F1= 95.52%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 95.13% | Macro-F1= 95.10%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 92.86% | Macro-F1= 89.95%\n",
      "  cid= 9 | N=  780 | ACC= 93.33% | Macro-F1= 92.46%\n",
      "  cid=10 | N=  254 | ACC= 96.06% | Macro-F1= 92.26%\n",
      "  cid=11 | N=  254 | ACC= 95.28% | Macro-F1= 95.00%\n",
      "[ROUND 028] VAL(QUERY) Per-Subject ACC = 95.85% Â± 3.08% | Macro-F1 = 94.62% Â± 4.04% | HyperLoss=0.000006 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round029) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.76%  (Std 3.34%)\n",
      "Mean Macro-F1 : 95.95%  (Std 3.66%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 97.73% | Macro-F1= 96.71%\n",
      "  cid= 2 | N=  321 | ACC= 89.10% | Macro-F1= 87.70%\n",
      "  cid= 3 | N=  182 | ACC= 97.80% | Macro-F1= 96.46%\n",
      "  cid= 4 | N=  222 | ACC= 95.05% | Macro-F1= 93.39%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 92.35% | Macro-F1= 92.17%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 96.64% | Macro-F1= 94.94%\n",
      "  cid= 9 | N=  780 | ACC= 95.26% | Macro-F1= 94.45%\n",
      "  cid=10 | N=  254 | ACC= 98.82% | Macro-F1= 97.48%\n",
      "  cid=11 | N=  254 | ACC= 98.82% | Macro-F1= 98.72%\n",
      "[ROUND 029] VAL(QUERY) Per-Subject ACC = 96.76% Â± 3.34% | Macro-F1 = 95.95% Â± 3.66% | HyperLoss=0.000004 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round030) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.38%  (Std 2.71%)\n",
      "Mean Macro-F1 : 95.29%  (Std 3.44%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.73% | Macro-F1= 96.73%\n",
      "  cid= 2 | N=  321 | ACC= 91.59% | Macro-F1= 90.06%\n",
      "  cid= 3 | N=  182 | ACC= 95.05% | Macro-F1= 92.59%\n",
      "  cid= 4 | N=  222 | ACC= 96.40% | Macro-F1= 95.52%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 94.09% | Macro-F1= 94.02%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 94.12% | Macro-F1= 91.50%\n",
      "  cid= 9 | N=  780 | ACC= 94.23% | Macro-F1= 93.41%\n",
      "  cid=10 | N=  254 | ACC= 96.46% | Macro-F1= 92.96%\n",
      "  cid=11 | N=  254 | ACC= 96.85% | Macro-F1= 96.64%\n",
      "[ROUND 030] VAL(QUERY) Per-Subject ACC = 96.38% Â± 2.71% | Macro-F1 = 95.29% Â± 3.44% | HyperLoss=0.000007 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round031) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.86%  (Std 2.80%)\n",
      "Mean Macro-F1 : 95.92%  (Std 3.35%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.35% | Macro-F1= 96.16%\n",
      "  cid= 2 | N=  321 | ACC= 90.34% | Macro-F1= 88.89%\n",
      "  cid= 3 | N=  182 | ACC= 97.25% | Macro-F1= 95.63%\n",
      "  cid= 4 | N=  222 | ACC= 97.75% | Macro-F1= 97.13%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 94.43% | Macro-F1= 94.37%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 95.17% | Macro-F1= 92.95%\n",
      "  cid= 9 | N=  780 | ACC= 95.13% | Macro-F1= 94.33%\n",
      "  cid=10 | N=  254 | ACC= 96.85% | Macro-F1= 93.67%\n",
      "  cid=11 | N=  254 | ACC= 98.03% | Macro-F1= 97.88%\n",
      "[ROUND 031] VAL(QUERY) Per-Subject ACC = 96.86% Â± 2.80% | Macro-F1 = 95.92% Â± 3.35% | HyperLoss=0.000007 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round032) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.06%  (Std 3.01%)\n",
      "Mean Macro-F1 : 96.26%  (Std 3.39%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.73% | Macro-F1= 96.69%\n",
      "  cid= 2 | N=  321 | ACC= 90.34% | Macro-F1= 88.95%\n",
      "  cid= 3 | N=  182 | ACC= 98.90% | Macro-F1= 98.19%\n",
      "  cid= 4 | N=  222 | ACC= 96.85% | Macro-F1= 95.94%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 92.87% | Macro-F1= 92.76%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 96.01% | Macro-F1= 94.05%\n",
      "  cid= 9 | N=  780 | ACC= 95.51% | Macro-F1= 94.73%\n",
      "  cid=10 | N=  254 | ACC= 97.64% | Macro-F1= 95.15%\n",
      "  cid=11 | N=  254 | ACC= 98.82% | Macro-F1= 98.72%\n",
      "[ROUND 032] VAL(QUERY) Per-Subject ACC = 97.06% Â± 3.01% | Macro-F1 = 96.26% Â± 3.39% | HyperLoss=0.000006 | p_rate=0.591\n",
      "ðŸ”¥ NEW BEST (by VAL/QUERY) @ Round 32 | Acc=97.06% | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round033) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.23%  (Std 2.86%)\n",
      "Mean Macro-F1 : 96.53%  (Std 3.19%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 98.49% | Macro-F1= 97.81%\n",
      "  cid= 2 | N=  321 | ACC= 90.97% | Macro-F1= 89.55%\n",
      "  cid= 3 | N=  182 | ACC= 98.35% | Macro-F1= 97.32%\n",
      "  cid= 4 | N=  222 | ACC= 96.40% | Macro-F1= 95.33%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.04% | Macro-F1= 92.94%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 96.22% | Macro-F1= 94.30%\n",
      "  cid= 9 | N=  780 | ACC= 96.03% | Macro-F1= 95.33%\n",
      "  cid=10 | N=  254 | ACC= 98.82% | Macro-F1= 97.48%\n",
      "  cid=11 | N=  254 | ACC= 98.43% | Macro-F1= 98.30%\n",
      "[ROUND 033] VAL(QUERY) Per-Subject ACC = 97.23% Â± 2.86% | Macro-F1 = 96.53% Â± 3.19% | HyperLoss=0.000005 | p_rate=0.591\n",
      "ðŸ”¥ NEW BEST (by VAL/QUERY) @ Round 33 | Acc=97.23% | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round034) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.85%  (Std 2.58%)\n",
      "Mean Macro-F1 : 95.94%  (Std 3.08%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.92% | Macro-F1= 97.01%\n",
      "  cid= 2 | N=  321 | ACC= 91.90% | Macro-F1= 90.51%\n",
      "  cid= 3 | N=  182 | ACC= 97.25% | Macro-F1= 95.72%\n",
      "  cid= 4 | N=  222 | ACC= 97.75% | Macro-F1= 97.16%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.39% | Macro-F1= 93.31%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 95.17% | Macro-F1= 92.90%\n",
      "  cid= 9 | N=  780 | ACC= 95.51% | Macro-F1= 94.82%\n",
      "  cid=10 | N=  254 | ACC= 96.85% | Macro-F1= 93.67%\n",
      "  cid=11 | N=  254 | ACC= 96.46% | Macro-F1= 96.22%\n",
      "[ROUND 034] VAL(QUERY) Per-Subject ACC = 96.85% Â± 2.58% | Macro-F1 = 95.94% Â± 3.08% | HyperLoss=0.000007 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round035) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.87%  (Std 3.03%)\n",
      "Mean Macro-F1 : 96.06%  (Std 3.37%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.35% | Macro-F1= 96.14%\n",
      "  cid= 2 | N=  321 | ACC= 90.65% | Macro-F1= 89.38%\n",
      "  cid= 3 | N=  182 | ACC= 98.90% | Macro-F1= 98.23%\n",
      "  cid= 4 | N=  222 | ACC= 95.95% | Macro-F1= 94.72%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 92.17% | Macro-F1= 92.00%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 95.59% | Macro-F1= 93.52%\n",
      "  cid= 9 | N=  780 | ACC= 95.77% | Macro-F1= 95.04%\n",
      "  cid=10 | N=  254 | ACC= 98.03% | Macro-F1= 95.81%\n",
      "  cid=11 | N=  254 | ACC= 98.03% | Macro-F1= 97.88%\n",
      "[ROUND 035] VAL(QUERY) Per-Subject ACC = 96.87% Â± 3.03% | Macro-F1 = 96.06% Â± 3.37% | HyperLoss=0.000006 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round036) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.14%  (Std 2.33%)\n",
      "Mean Macro-F1 : 96.37%  (Std 2.70%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 97.92% | Macro-F1= 96.99%\n",
      "  cid= 2 | N=  321 | ACC= 92.21% | Macro-F1= 90.99%\n",
      "  cid= 3 | N=  182 | ACC= 96.70% | Macro-F1= 94.81%\n",
      "  cid= 4 | N=  222 | ACC= 97.30% | Macro-F1= 96.57%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 94.61% | Macro-F1= 94.54%\n",
      "  cid= 7 | N=  113 | ACC= 99.12% | Macro-F1= 99.11%\n",
      "  cid= 8 | N=  476 | ACC= 95.59% | Macro-F1= 93.56%\n",
      "  cid= 9 | N=  780 | ACC= 95.38% | Macro-F1= 94.67%\n",
      "  cid=10 | N=  254 | ACC= 98.82% | Macro-F1= 97.48%\n",
      "  cid=11 | N=  254 | ACC= 98.43% | Macro-F1= 98.30%\n",
      "[ROUND 036] VAL(QUERY) Per-Subject ACC = 97.14% Â± 2.33% | Macro-F1 = 96.37% Â± 2.70% | HyperLoss=0.000006 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round037) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.21%  (Std 2.81%)\n",
      "Mean Macro-F1 : 96.51%  (Std 3.14%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.92% | Macro-F1= 96.97%\n",
      "  cid= 2 | N=  321 | ACC= 90.97% | Macro-F1= 89.71%\n",
      "  cid= 3 | N=  182 | ACC= 98.35% | Macro-F1= 97.32%\n",
      "  cid= 4 | N=  222 | ACC= 96.40% | Macro-F1= 95.33%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.22% | Macro-F1= 93.10%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 95.80% | Macro-F1= 93.76%\n",
      "  cid= 9 | N=  780 | ACC= 96.67% | Macro-F1= 96.11%\n",
      "  cid=10 | N=  254 | ACC= 98.82% | Macro-F1= 97.48%\n",
      "  cid=11 | N=  254 | ACC= 98.43% | Macro-F1= 98.30%\n",
      "[ROUND 037] VAL(QUERY) Per-Subject ACC = 97.21% Â± 2.81% | Macro-F1 = 96.51% Â± 3.14% | HyperLoss=0.000007 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round038) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.18%  (Std 2.81%)\n",
      "Mean Macro-F1 : 96.43%  (Std 3.22%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.73% | Macro-F1= 96.71%\n",
      "  cid= 2 | N=  321 | ACC= 90.34% | Macro-F1= 88.83%\n",
      "  cid= 3 | N=  182 | ACC= 98.90% | Macro-F1= 98.23%\n",
      "  cid= 4 | N=  222 | ACC= 97.30% | Macro-F1= 96.54%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 94.09% | Macro-F1= 94.00%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 95.80% | Macro-F1= 93.85%\n",
      "  cid= 9 | N=  780 | ACC= 96.28% | Macro-F1= 95.67%\n",
      "  cid=10 | N=  254 | ACC= 98.03% | Macro-F1= 95.91%\n",
      "  cid=11 | N=  254 | ACC= 97.64% | Macro-F1= 97.47%\n",
      "[ROUND 038] VAL(QUERY) Per-Subject ACC = 97.18% Â± 2.81% | Macro-F1 = 96.43% Â± 3.22% | HyperLoss=0.000006 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round039) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.02%  (Std 2.69%)\n",
      "Mean Macro-F1 : 96.19%  (Std 3.14%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.35% | Macro-F1= 96.16%\n",
      "  cid= 2 | N=  321 | ACC= 91.59% | Macro-F1= 90.27%\n",
      "  cid= 3 | N=  182 | ACC= 98.35% | Macro-F1= 97.38%\n",
      "  cid= 4 | N=  222 | ACC= 97.30% | Macro-F1= 96.57%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.91% | Macro-F1= 93.83%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 94.96% | Macro-F1= 92.72%\n",
      "  cid= 9 | N=  780 | ACC= 94.74% | Macro-F1= 93.93%\n",
      "  cid=10 | N=  254 | ACC= 97.64% | Macro-F1= 95.15%\n",
      "  cid=11 | N=  254 | ACC= 98.43% | Macro-F1= 98.30%\n",
      "[ROUND 039] VAL(QUERY) Per-Subject ACC = 97.02% Â± 2.69% | Macro-F1 = 96.19% Â± 3.14% | HyperLoss=0.000006 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round040) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.21%  (Std 2.64%)\n",
      "Mean Macro-F1 : 96.50%  (Std 2.97%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.54% | Macro-F1= 96.44%\n",
      "  cid= 2 | N=  321 | ACC= 91.59% | Macro-F1= 90.37%\n",
      "  cid= 3 | N=  182 | ACC= 98.35% | Macro-F1= 97.32%\n",
      "  cid= 4 | N=  222 | ACC= 96.85% | Macro-F1= 95.94%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.57% | Macro-F1= 93.47%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 95.59% | Macro-F1= 93.56%\n",
      "  cid= 9 | N=  780 | ACC= 96.15% | Macro-F1= 95.51%\n",
      "  cid=10 | N=  254 | ACC= 98.82% | Macro-F1= 97.48%\n",
      "  cid=11 | N=  254 | ACC= 98.03% | Macro-F1= 97.88%\n",
      "[ROUND 040] VAL(QUERY) Per-Subject ACC = 97.21% Â± 2.64% | Macro-F1 = 96.50% Â± 2.97% | HyperLoss=0.000006 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round041) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.97%  (Std 2.67%)\n",
      "Mean Macro-F1 : 96.11%  (Std 3.15%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.73% | Macro-F1= 96.71%\n",
      "  cid= 2 | N=  321 | ACC= 91.28% | Macro-F1= 89.83%\n",
      "  cid= 3 | N=  182 | ACC= 96.70% | Macro-F1= 94.91%\n",
      "  cid= 4 | N=  222 | ACC= 97.30% | Macro-F1= 96.54%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.74% | Macro-F1= 93.65%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 94.96% | Macro-F1= 92.62%\n",
      "  cid= 9 | N=  780 | ACC= 95.90% | Macro-F1= 95.26%\n",
      "  cid=10 | N=  254 | ACC= 98.03% | Macro-F1= 95.91%\n",
      "  cid=11 | N=  254 | ACC= 98.03% | Macro-F1= 97.88%\n",
      "[ROUND 041] VAL(QUERY) Per-Subject ACC = 96.97% Â± 2.67% | Macro-F1 = 96.11% Â± 3.15% | HyperLoss=0.000008 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round042) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.79%  (Std 2.85%)\n",
      "Mean Macro-F1 : 95.85%  (Std 3.22%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 97.35% | Macro-F1= 96.06%\n",
      "  cid= 2 | N=  321 | ACC= 90.97% | Macro-F1= 89.76%\n",
      "  cid= 3 | N=  182 | ACC= 96.15% | Macro-F1= 93.88%\n",
      "  cid= 4 | N=  222 | ACC= 96.40% | Macro-F1= 95.33%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 92.52% | Macro-F1= 92.37%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 95.59% | Macro-F1= 93.47%\n",
      "  cid= 9 | N=  780 | ACC= 96.03% | Macro-F1= 95.33%\n",
      "  cid=10 | N=  254 | ACC= 98.03% | Macro-F1= 95.91%\n",
      "  cid=11 | N=  254 | ACC= 98.82% | Macro-F1= 98.72%\n",
      "[ROUND 042] VAL(QUERY) Per-Subject ACC = 96.79% Â± 2.85% | Macro-F1 = 95.85% Â± 3.22% | HyperLoss=0.000006 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round043) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.79%  (Std 2.81%)\n",
      "Mean Macro-F1 : 95.90%  (Std 3.33%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.54% | Macro-F1= 96.47%\n",
      "  cid= 2 | N=  321 | ACC= 90.97% | Macro-F1= 89.50%\n",
      "  cid= 3 | N=  182 | ACC= 96.70% | Macro-F1= 94.91%\n",
      "  cid= 4 | N=  222 | ACC= 97.30% | Macro-F1= 96.57%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.57% | Macro-F1= 93.47%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 94.12% | Macro-F1= 91.50%\n",
      "  cid= 9 | N=  780 | ACC= 95.64% | Macro-F1= 94.96%\n",
      "  cid=10 | N=  254 | ACC= 98.03% | Macro-F1= 95.91%\n",
      "  cid=11 | N=  254 | ACC= 97.64% | Macro-F1= 97.47%\n",
      "[ROUND 043] VAL(QUERY) Per-Subject ACC = 96.79% Â± 2.81% | Macro-F1 = 95.90% Â± 3.33% | HyperLoss=0.000004 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round044) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.81%  (Std 3.01%)\n",
      "Mean Macro-F1 : 95.92%  (Std 3.54%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.73% | Macro-F1= 96.73%\n",
      "  cid= 2 | N=  321 | ACC= 89.72% | Macro-F1= 88.18%\n",
      "  cid= 3 | N=  182 | ACC= 96.15% | Macro-F1= 94.00%\n",
      "  cid= 4 | N=  222 | ACC= 98.20% | Macro-F1= 97.72%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 94.26% | Macro-F1= 94.21%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 94.75% | Macro-F1= 92.34%\n",
      "  cid= 9 | N=  780 | ACC= 95.26% | Macro-F1= 94.55%\n",
      "  cid=10 | N=  254 | ACC= 98.03% | Macro-F1= 95.91%\n",
      "  cid=11 | N=  254 | ACC= 97.64% | Macro-F1= 97.47%\n",
      "[ROUND 044] VAL(QUERY) Per-Subject ACC = 96.81% Â± 3.01% | Macro-F1 = 95.92% Â± 3.54% | HyperLoss=0.000006 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round045) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.00%  (Std 3.25%)\n",
      "Mean Macro-F1 : 96.30%  (Std 3.54%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.73% | Macro-F1= 96.73%\n",
      "  cid= 2 | N=  321 | ACC= 89.10% | Macro-F1= 87.76%\n",
      "  cid= 3 | N=  182 | ACC= 98.35% | Macro-F1= 97.32%\n",
      "  cid= 4 | N=  222 | ACC= 96.40% | Macro-F1= 95.33%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 92.87% | Macro-F1= 92.73%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 96.43% | Macro-F1= 94.72%\n",
      "  cid= 9 | N=  780 | ACC= 95.90% | Macro-F1= 95.22%\n",
      "  cid=10 | N=  254 | ACC= 98.82% | Macro-F1= 97.48%\n",
      "  cid=11 | N=  254 | ACC= 98.43% | Macro-F1= 98.30%\n",
      "[ROUND 045] VAL(QUERY) Per-Subject ACC = 97.00% Â± 3.25% | Macro-F1 = 96.30% Â± 3.54% | HyperLoss=0.000006 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round046) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.05%  (Std 2.71%)\n",
      "Mean Macro-F1 : 96.23%  (Std 3.19%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.92% | Macro-F1= 96.99%\n",
      "  cid= 2 | N=  321 | ACC= 90.97% | Macro-F1= 89.66%\n",
      "  cid= 3 | N=  182 | ACC= 95.60% | Macro-F1= 93.07%\n",
      "  cid= 4 | N=  222 | ACC= 96.85% | Macro-F1= 95.94%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 94.61% | Macro-F1= 94.54%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 96.01% | Macro-F1= 94.09%\n",
      "  cid= 9 | N=  780 | ACC= 95.38% | Macro-F1= 94.67%\n",
      "  cid=10 | N=  254 | ACC= 98.82% | Macro-F1= 97.48%\n",
      "  cid=11 | N=  254 | ACC= 98.43% | Macro-F1= 98.30%\n",
      "[ROUND 046] VAL(QUERY) Per-Subject ACC = 97.05% Â± 2.71% | Macro-F1 = 96.23% Â± 3.19% | HyperLoss=0.000006 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round047) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.11%  (Std 3.15%)\n",
      "Mean Macro-F1 : 96.41%  (Std 3.47%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.73% | Macro-F1= 96.71%\n",
      "  cid= 2 | N=  321 | ACC= 89.41% | Macro-F1= 88.08%\n",
      "  cid= 3 | N=  182 | ACC= 97.80% | Macro-F1= 96.46%\n",
      "  cid= 4 | N=  222 | ACC= 98.20% | Macro-F1= 97.69%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.22% | Macro-F1= 93.12%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 96.01% | Macro-F1= 94.05%\n",
      "  cid= 9 | N=  780 | ACC= 95.77% | Macro-F1= 95.06%\n",
      "  cid=10 | N=  254 | ACC= 98.82% | Macro-F1= 97.48%\n",
      "  cid=11 | N=  254 | ACC= 98.43% | Macro-F1= 98.30%\n",
      "[ROUND 047] VAL(QUERY) Per-Subject ACC = 97.11% Â± 3.15% | Macro-F1 = 96.41% Â± 3.47% | HyperLoss=0.000005 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round048) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.24%  (Std 2.54%)\n",
      "Mean Macro-F1 : 96.47%  (Std 2.98%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.73% | Macro-F1= 96.71%\n",
      "  cid= 2 | N=  321 | ACC= 91.59% | Macro-F1= 90.33%\n",
      "  cid= 3 | N=  182 | ACC= 97.80% | Macro-F1= 96.54%\n",
      "  cid= 4 | N=  222 | ACC= 98.20% | Macro-F1= 97.72%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 94.61% | Macro-F1= 94.55%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 95.17% | Macro-F1= 92.90%\n",
      "  cid= 9 | N=  780 | ACC= 95.77% | Macro-F1= 95.09%\n",
      "  cid=10 | N=  254 | ACC= 98.03% | Macro-F1= 95.91%\n",
      "  cid=11 | N=  254 | ACC= 98.03% | Macro-F1= 97.88%\n",
      "[ROUND 048] VAL(QUERY) Per-Subject ACC = 97.24% Â± 2.54% | Macro-F1 = 96.47% Â± 2.98% | HyperLoss=0.000009 | p_rate=0.591\n",
      "ðŸ”¥ NEW BEST (by VAL/QUERY) @ Round 48 | Acc=97.24% | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round049) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.21%  (Std 3.09%)\n",
      "Mean Macro-F1 : 96.57%  (Std 3.42%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.73% | Macro-F1= 96.67%\n",
      "  cid= 2 | N=  321 | ACC= 90.03% | Macro-F1= 88.68%\n",
      "  cid= 3 | N=  182 | ACC= 98.90% | Macro-F1= 98.23%\n",
      "  cid= 4 | N=  222 | ACC= 97.30% | Macro-F1= 96.50%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 92.87% | Macro-F1= 92.72%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 96.01% | Macro-F1= 94.05%\n",
      "  cid= 9 | N=  780 | ACC= 96.03% | Macro-F1= 95.34%\n",
      "  cid=10 | N=  254 | ACC= 99.21% | Macro-F1= 98.30%\n",
      "  cid=11 | N=  254 | ACC= 98.43% | Macro-F1= 98.30%\n",
      "[ROUND 049] VAL(QUERY) Per-Subject ACC = 97.21% Â± 3.09% | Macro-F1 = 96.57% Â± 3.42% | HyperLoss=0.000006 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round050) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.97%  (Std 2.51%)\n",
      "Mean Macro-F1 : 96.15%  (Std 2.98%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 98.11% | Macro-F1= 97.29%\n",
      "  cid= 2 | N=  321 | ACC= 92.21% | Macro-F1= 90.99%\n",
      "  cid= 3 | N=  182 | ACC= 96.70% | Macro-F1= 94.91%\n",
      "  cid= 4 | N=  222 | ACC= 97.75% | Macro-F1= 97.16%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 94.61% | Macro-F1= 94.57%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 94.54% | Macro-F1= 92.16%\n",
      "  cid= 9 | N=  780 | ACC= 94.87% | Macro-F1= 94.13%\n",
      "  cid=10 | N=  254 | ACC= 98.03% | Macro-F1= 95.91%\n",
      "  cid=11 | N=  254 | ACC= 96.85% | Macro-F1= 96.64%\n",
      "[ROUND 050] VAL(QUERY) Per-Subject ACC = 96.97% Â± 2.51% | Macro-F1 = 96.15% Â± 2.98% | HyperLoss=0.000007 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round051) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.21%  (Std 2.71%)\n",
      "Mean Macro-F1 : 96.46%  (Std 3.07%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.73% | Macro-F1= 96.71%\n",
      "  cid= 2 | N=  321 | ACC= 90.97% | Macro-F1= 89.71%\n",
      "  cid= 3 | N=  182 | ACC= 97.80% | Macro-F1= 96.54%\n",
      "  cid= 4 | N=  222 | ACC= 97.75% | Macro-F1= 97.13%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.74% | Macro-F1= 93.63%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 95.59% | Macro-F1= 93.56%\n",
      "  cid= 9 | N=  780 | ACC= 96.54% | Macro-F1= 95.97%\n",
      "  cid=10 | N=  254 | ACC= 98.03% | Macro-F1= 95.91%\n",
      "  cid=11 | N=  254 | ACC= 98.43% | Macro-F1= 98.30%\n",
      "[ROUND 051] VAL(QUERY) Per-Subject ACC = 97.21% Â± 2.71% | Macro-F1 = 96.46% Â± 3.07% | HyperLoss=0.000006 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round052) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.04%  (Std 2.64%)\n",
      "Mean Macro-F1 : 96.22%  (Std 3.09%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.92% | Macro-F1= 96.99%\n",
      "  cid= 2 | N=  321 | ACC= 91.90% | Macro-F1= 90.66%\n",
      "  cid= 3 | N=  182 | ACC= 97.25% | Macro-F1= 95.72%\n",
      "  cid= 4 | N=  222 | ACC= 97.75% | Macro-F1= 97.13%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.57% | Macro-F1= 93.46%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 94.54% | Macro-F1= 92.11%\n",
      "  cid= 9 | N=  780 | ACC= 95.51% | Macro-F1= 94.81%\n",
      "  cid=10 | N=  254 | ACC= 98.03% | Macro-F1= 95.91%\n",
      "  cid=11 | N=  254 | ACC= 98.03% | Macro-F1= 97.88%\n",
      "[ROUND 052] VAL(QUERY) Per-Subject ACC = 97.04% Â± 2.64% | Macro-F1 = 96.22% Â± 3.09% | HyperLoss=0.000009 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round053) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.25%  (Std 2.70%)\n",
      "Mean Macro-F1 : 96.55%  (Std 3.02%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.73% | Macro-F1= 96.73%\n",
      "  cid= 2 | N=  321 | ACC= 91.90% | Macro-F1= 90.66%\n",
      "  cid= 3 | N=  182 | ACC= 98.90% | Macro-F1= 98.23%\n",
      "  cid= 4 | N=  222 | ACC= 98.20% | Macro-F1= 97.72%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 92.70% | Macro-F1= 92.58%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 95.80% | Macro-F1= 93.85%\n",
      "  cid= 9 | N=  780 | ACC= 96.15% | Macro-F1= 95.53%\n",
      "  cid=10 | N=  254 | ACC= 98.03% | Macro-F1= 95.91%\n",
      "  cid=11 | N=  254 | ACC= 97.64% | Macro-F1= 97.47%\n",
      "[ROUND 053] VAL(QUERY) Per-Subject ACC = 97.25% Â± 2.70% | Macro-F1 = 96.55% Â± 3.02% | HyperLoss=0.000008 | p_rate=0.591\n",
      "ðŸ”¥ NEW BEST (by VAL/QUERY) @ Round 53 | Acc=97.25% | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round054) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.00%  (Std 2.58%)\n",
      "Mean Macro-F1 : 96.22%  (Std 2.97%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.73% | Macro-F1= 96.73%\n",
      "  cid= 2 | N=  321 | ACC= 91.90% | Macro-F1= 90.66%\n",
      "  cid= 3 | N=  182 | ACC= 97.80% | Macro-F1= 96.54%\n",
      "  cid= 4 | N=  222 | ACC= 98.20% | Macro-F1= 97.72%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 94.26% | Macro-F1= 94.21%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 94.75% | Macro-F1= 92.54%\n",
      "  cid= 9 | N=  780 | ACC= 95.26% | Macro-F1= 94.56%\n",
      "  cid=10 | N=  254 | ACC= 98.03% | Macro-F1= 95.91%\n",
      "  cid=11 | N=  254 | ACC= 96.06% | Macro-F1= 95.81%\n",
      "[ROUND 054] VAL(QUERY) Per-Subject ACC = 97.00% Â± 2.58% | Macro-F1 = 96.22% Â± 2.97% | HyperLoss=0.000006 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round055) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.13%  (Std 2.92%)\n",
      "Mean Macro-F1 : 96.47%  (Std 3.19%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.54% | Macro-F1= 96.42%\n",
      "  cid= 2 | N=  321 | ACC= 90.97% | Macro-F1= 89.71%\n",
      "  cid= 3 | N=  182 | ACC= 98.90% | Macro-F1= 98.19%\n",
      "  cid= 4 | N=  222 | ACC= 97.75% | Macro-F1= 97.13%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 92.52% | Macro-F1= 92.42%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 95.80% | Macro-F1= 93.89%\n",
      "  cid= 9 | N=  780 | ACC= 96.03% | Macro-F1= 95.35%\n",
      "  cid=10 | N=  254 | ACC= 98.82% | Macro-F1= 97.48%\n",
      "  cid=11 | N=  254 | ACC= 97.24% | Macro-F1= 97.05%\n",
      "[ROUND 055] VAL(QUERY) Per-Subject ACC = 97.13% Â± 2.92% | Macro-F1 = 96.47% Â± 3.19% | HyperLoss=0.000006 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round056) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.26%  (Std 2.32%)\n",
      "Mean Macro-F1 : 96.45%  (Std 2.80%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 98.11% | Macro-F1= 97.29%\n",
      "  cid= 2 | N=  321 | ACC= 92.52% | Macro-F1= 91.33%\n",
      "  cid= 3 | N=  182 | ACC= 97.25% | Macro-F1= 95.63%\n",
      "  cid= 4 | N=  222 | ACC= 98.20% | Macro-F1= 97.72%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 94.96% | Macro-F1= 94.91%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 95.17% | Macro-F1= 93.00%\n",
      "  cid= 9 | N=  780 | ACC= 95.64% | Macro-F1= 94.95%\n",
      "  cid=10 | N=  254 | ACC= 97.64% | Macro-F1= 95.15%\n",
      "  cid=11 | N=  254 | ACC= 97.64% | Macro-F1= 97.47%\n",
      "[ROUND 056] VAL(QUERY) Per-Subject ACC = 97.26% Â± 2.32% | Macro-F1 = 96.45% Â± 2.80% | HyperLoss=0.000009 | p_rate=0.591\n",
      "ðŸ”¥ NEW BEST (by VAL/QUERY) @ Round 56 | Acc=97.26% | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round057) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.99%  (Std 2.43%)\n",
      "Mean Macro-F1 : 96.03%  (Std 3.13%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 98.11% | Macro-F1= 97.29%\n",
      "  cid= 2 | N=  321 | ACC= 92.52% | Macro-F1= 91.29%\n",
      "  cid= 3 | N=  182 | ACC= 94.51% | Macro-F1= 91.68%\n",
      "  cid= 4 | N=  222 | ACC= 98.20% | Macro-F1= 97.74%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 94.78% | Macro-F1= 94.74%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 94.96% | Macro-F1= 92.77%\n",
      "  cid= 9 | N=  780 | ACC= 95.90% | Macro-F1= 95.25%\n",
      "  cid=10 | N=  254 | ACC= 97.24% | Macro-F1= 94.40%\n",
      "  cid=11 | N=  254 | ACC= 98.03% | Macro-F1= 97.88%\n",
      "[ROUND 057] VAL(QUERY) Per-Subject ACC = 96.99% Â± 2.43% | Macro-F1 = 96.03% Â± 3.13% | HyperLoss=0.000010 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round058) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.07%  (Std 2.95%)\n",
      "Mean Macro-F1 : 96.32%  (Std 3.22%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 97.35% | Macro-F1= 96.16%\n",
      "  cid= 2 | N=  321 | ACC= 90.34% | Macro-F1= 89.00%\n",
      "  cid= 3 | N=  182 | ACC= 97.80% | Macro-F1= 96.38%\n",
      "  cid= 4 | N=  222 | ACC= 96.40% | Macro-F1= 95.33%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 92.70% | Macro-F1= 92.57%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 96.64% | Macro-F1= 95.01%\n",
      "  cid= 9 | N=  780 | ACC= 96.41% | Macro-F1= 95.79%\n",
      "  cid=10 | N=  254 | ACC= 98.82% | Macro-F1= 97.48%\n",
      "  cid=11 | N=  254 | ACC= 98.82% | Macro-F1= 98.72%\n",
      "[ROUND 058] VAL(QUERY) Per-Subject ACC = 97.07% Â± 2.95% | Macro-F1 = 96.32% Â± 3.22% | HyperLoss=0.000006 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round059) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.04%  (Std 2.22%)\n",
      "Mean Macro-F1 : 96.19%  (Std 2.69%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.92% | Macro-F1= 97.03%\n",
      "  cid= 2 | N=  321 | ACC= 92.52% | Macro-F1= 91.29%\n",
      "  cid= 3 | N=  182 | ACC= 96.15% | Macro-F1= 94.00%\n",
      "  cid= 4 | N=  222 | ACC= 97.75% | Macro-F1= 97.16%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 94.43% | Macro-F1= 94.39%\n",
      "  cid= 7 | N=  113 | ACC= 99.12% | Macro-F1= 99.11%\n",
      "  cid= 8 | N=  476 | ACC= 95.80% | Macro-F1= 93.89%\n",
      "  cid= 9 | N=  780 | ACC= 95.90% | Macro-F1= 95.24%\n",
      "  cid=10 | N=  254 | ACC= 97.64% | Macro-F1= 95.15%\n",
      "  cid=11 | N=  254 | ACC= 97.24% | Macro-F1= 97.05%\n",
      "[ROUND 059] VAL(QUERY) Per-Subject ACC = 97.04% Â± 2.22% | Macro-F1 = 96.19% Â± 2.69% | HyperLoss=0.000007 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round060) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.21%  (Std 2.97%)\n",
      "Mean Macro-F1 : 96.53%  (Std 3.15%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.19% | Macro-F1= 98.69%\n",
      "  cid= 1 | N=  529 | ACC= 98.49% | Macro-F1= 97.78%\n",
      "  cid= 2 | N=  321 | ACC= 90.97% | Macro-F1= 89.81%\n",
      "  cid= 3 | N=  182 | ACC= 98.35% | Macro-F1= 97.26%\n",
      "  cid= 4 | N=  222 | ACC= 97.30% | Macro-F1= 96.54%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 91.83% | Macro-F1= 91.66%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 96.64% | Macro-F1= 94.97%\n",
      "  cid= 9 | N=  780 | ACC= 96.15% | Macro-F1= 95.48%\n",
      "  cid=10 | N=  254 | ACC= 98.82% | Macro-F1= 97.48%\n",
      "  cid=11 | N=  254 | ACC= 98.82% | Macro-F1= 98.72%\n",
      "[ROUND 060] VAL(QUERY) Per-Subject ACC = 97.21% Â± 2.97% | Macro-F1 = 96.53% Â± 3.15% | HyperLoss=0.000007 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round061) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.30%  (Std 2.49%)\n",
      "Mean Macro-F1 : 96.54%  (Std 2.86%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.73% | Macro-F1= 96.73%\n",
      "  cid= 2 | N=  321 | ACC= 92.21% | Macro-F1= 90.99%\n",
      "  cid= 3 | N=  182 | ACC= 97.80% | Macro-F1= 96.46%\n",
      "  cid= 4 | N=  222 | ACC= 98.20% | Macro-F1= 97.72%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.91% | Macro-F1= 93.83%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 95.59% | Macro-F1= 93.61%\n",
      "  cid= 9 | N=  780 | ACC= 95.64% | Macro-F1= 94.94%\n",
      "  cid=10 | N=  254 | ACC= 98.03% | Macro-F1= 95.91%\n",
      "  cid=11 | N=  254 | ACC= 98.43% | Macro-F1= 98.30%\n",
      "[ROUND 061] VAL(QUERY) Per-Subject ACC = 97.30% Â± 2.49% | Macro-F1 = 96.54% Â± 2.86% | HyperLoss=0.000006 | p_rate=0.591\n",
      "ðŸ”¥ NEW BEST (by VAL/QUERY) @ Round 61 | Acc=97.30% | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round062) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.73%  (Std 2.85%)\n",
      "Mean Macro-F1 : 95.83%  (Std 3.42%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.73% | Macro-F1= 96.73%\n",
      "  cid= 2 | N=  321 | ACC= 90.97% | Macro-F1= 89.50%\n",
      "  cid= 3 | N=  182 | ACC= 96.15% | Macro-F1= 94.12%\n",
      "  cid= 4 | N=  222 | ACC= 98.20% | Macro-F1= 97.72%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 94.61% | Macro-F1= 94.56%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 93.70% | Macro-F1= 91.13%\n",
      "  cid= 9 | N=  780 | ACC= 94.49% | Macro-F1= 93.69%\n",
      "  cid=10 | N=  254 | ACC= 98.03% | Macro-F1= 95.91%\n",
      "  cid=11 | N=  254 | ACC= 96.85% | Macro-F1= 96.64%\n",
      "[ROUND 062] VAL(QUERY) Per-Subject ACC = 96.73% Â± 2.85% | Macro-F1 = 95.83% Â± 3.42% | HyperLoss=0.000007 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round063) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.07%  (Std 3.23%)\n",
      "Mean Macro-F1 : 96.39%  (Std 3.56%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 97.92% | Macro-F1= 96.99%\n",
      "  cid= 2 | N=  321 | ACC= 89.41% | Macro-F1= 88.14%\n",
      "  cid= 3 | N=  182 | ACC= 97.25% | Macro-F1= 95.63%\n",
      "  cid= 4 | N=  222 | ACC= 97.75% | Macro-F1= 97.10%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 92.70% | Macro-F1= 92.56%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 96.01% | Macro-F1= 94.05%\n",
      "  cid= 9 | N=  780 | ACC= 95.77% | Macro-F1= 95.05%\n",
      "  cid=10 | N=  254 | ACC= 99.61% | Macro-F1= 99.14%\n",
      "  cid=11 | N=  254 | ACC= 98.82% | Macro-F1= 98.72%\n",
      "[ROUND 063] VAL(QUERY) Per-Subject ACC = 97.07% Â± 3.23% | Macro-F1 = 96.39% Â± 3.56% | HyperLoss=0.000007 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round064) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.62%  (Std 2.66%)\n",
      "Mean Macro-F1 : 95.63%  (Std 3.34%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 97.92% | Macro-F1= 97.03%\n",
      "  cid= 2 | N=  321 | ACC= 91.90% | Macro-F1= 90.45%\n",
      "  cid= 3 | N=  182 | ACC= 94.51% | Macro-F1= 91.84%\n",
      "  cid= 4 | N=  222 | ACC= 97.75% | Macro-F1= 97.19%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 94.78% | Macro-F1= 94.74%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 93.70% | Macro-F1= 91.13%\n",
      "  cid= 9 | N=  780 | ACC= 94.74% | Macro-F1= 94.00%\n",
      "  cid=10 | N=  254 | ACC= 97.64% | Macro-F1= 95.15%\n",
      "  cid=11 | N=  254 | ACC= 96.85% | Macro-F1= 96.64%\n",
      "[ROUND 064] VAL(QUERY) Per-Subject ACC = 96.62% Â± 2.66% | Macro-F1 = 95.63% Â± 3.34% | HyperLoss=0.000008 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round065) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.01%  (Std 2.94%)\n",
      "Mean Macro-F1 : 96.38%  (Std 3.13%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.19% | Macro-F1= 98.69%\n",
      "  cid= 1 | N=  529 | ACC= 97.54% | Macro-F1= 96.42%\n",
      "  cid= 2 | N=  321 | ACC= 90.34% | Macro-F1= 89.11%\n",
      "  cid= 3 | N=  182 | ACC= 98.90% | Macro-F1= 98.19%\n",
      "  cid= 4 | N=  222 | ACC= 98.20% | Macro-F1= 97.72%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 92.35% | Macro-F1= 92.21%\n",
      "  cid= 7 | N=  113 | ACC= 97.35% | Macro-F1= 97.34%\n",
      "  cid= 8 | N=  476 | ACC= 96.64% | Macro-F1= 94.97%\n",
      "  cid= 9 | N=  780 | ACC= 95.64% | Macro-F1= 94.90%\n",
      "  cid=10 | N=  254 | ACC= 99.21% | Macro-F1= 98.30%\n",
      "  cid=11 | N=  254 | ACC= 98.82% | Macro-F1= 98.72%\n",
      "[ROUND 065] VAL(QUERY) Per-Subject ACC = 97.01% Â± 2.94% | Macro-F1 = 96.38% Â± 3.13% | HyperLoss=0.000007 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round066) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.71%  (Std 2.58%)\n",
      "Mean Macro-F1 : 95.78%  (Std 3.16%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 97.35% | Macro-F1= 96.18%\n",
      "  cid= 2 | N=  321 | ACC= 92.52% | Macro-F1= 91.24%\n",
      "  cid= 3 | N=  182 | ACC= 95.05% | Macro-F1= 92.59%\n",
      "  cid= 4 | N=  222 | ACC= 97.75% | Macro-F1= 97.16%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 94.09% | Macro-F1= 94.02%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 93.70% | Macro-F1= 91.08%\n",
      "  cid= 9 | N=  780 | ACC= 95.13% | Macro-F1= 94.42%\n",
      "  cid=10 | N=  254 | ACC= 98.43% | Macro-F1= 96.69%\n",
      "  cid=11 | N=  254 | ACC= 96.85% | Macro-F1= 96.64%\n",
      "[ROUND 066] VAL(QUERY) Per-Subject ACC = 96.71% Â± 2.58% | Macro-F1 = 95.78% Â± 3.16% | HyperLoss=0.000010 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round067) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.43%  (Std 2.88%)\n",
      "Mean Macro-F1 : 96.81%  (Std 3.14%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 97.73% | Macro-F1= 96.71%\n",
      "  cid= 2 | N=  321 | ACC= 90.97% | Macro-F1= 89.71%\n",
      "  cid= 3 | N=  182 | ACC= 98.35% | Macro-F1= 97.32%\n",
      "  cid= 4 | N=  222 | ACC= 98.65% | Macro-F1= 98.28%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 92.70% | Macro-F1= 92.56%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 96.43% | Macro-F1= 94.68%\n",
      "  cid= 9 | N=  780 | ACC= 96.67% | Macro-F1= 96.09%\n",
      "  cid=10 | N=  254 | ACC= 99.21% | Macro-F1= 98.30%\n",
      "  cid=11 | N=  254 | ACC= 98.82% | Macro-F1= 98.72%\n",
      "[ROUND 067] VAL(QUERY) Per-Subject ACC = 97.43% Â± 2.88% | Macro-F1 = 96.81% Â± 3.14% | HyperLoss=0.000006 | p_rate=0.591\n",
      "ðŸ”¥ NEW BEST (by VAL/QUERY) @ Round 67 | Acc=97.43% | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round068) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.06%  (Std 2.77%)\n",
      "Mean Macro-F1 : 96.24%  (Std 3.07%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.19% | Macro-F1= 98.69%\n",
      "  cid= 1 | N=  529 | ACC= 97.54% | Macro-F1= 96.42%\n",
      "  cid= 2 | N=  321 | ACC= 90.97% | Macro-F1= 89.61%\n",
      "  cid= 3 | N=  182 | ACC= 98.35% | Macro-F1= 97.38%\n",
      "  cid= 4 | N=  222 | ACC= 98.20% | Macro-F1= 97.72%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 92.70% | Macro-F1= 92.58%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 96.22% | Macro-F1= 94.46%\n",
      "  cid= 9 | N=  780 | ACC= 95.90% | Macro-F1= 95.21%\n",
      "  cid=10 | N=  254 | ACC= 97.64% | Macro-F1= 94.91%\n",
      "  cid=11 | N=  254 | ACC= 98.03% | Macro-F1= 97.88%\n",
      "[ROUND 068] VAL(QUERY) Per-Subject ACC = 97.06% Â± 2.77% | Macro-F1 = 96.24% Â± 3.07% | HyperLoss=0.000005 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round069) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.85%  (Std 2.35%)\n",
      "Mean Macro-F1 : 95.93%  (Std 2.90%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 97.73% | Macro-F1= 96.73%\n",
      "  cid= 2 | N=  321 | ACC= 92.83% | Macro-F1= 91.67%\n",
      "  cid= 3 | N=  182 | ACC= 95.05% | Macro-F1= 92.44%\n",
      "  cid= 4 | N=  222 | ACC= 98.20% | Macro-F1= 97.74%\n",
      "  cid= 5 | N=  141 | ACC= 99.29% | Macro-F1= 99.18%\n",
      "  cid= 6 | N=  575 | ACC= 94.26% | Macro-F1= 94.21%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 94.33% | Macro-F1= 91.94%\n",
      "  cid= 9 | N=  780 | ACC= 96.03% | Macro-F1= 95.40%\n",
      "  cid=10 | N=  254 | ACC= 98.03% | Macro-F1= 95.91%\n",
      "  cid=11 | N=  254 | ACC= 96.85% | Macro-F1= 96.64%\n",
      "[ROUND 069] VAL(QUERY) Per-Subject ACC = 96.85% Â± 2.35% | Macro-F1 = 95.93% Â± 2.90% | HyperLoss=0.000007 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round070) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.23%  (Std 3.00%)\n",
      "Mean Macro-F1 : 96.60%  (Std 3.32%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 97.54% | Macro-F1= 96.42%\n",
      "  cid= 2 | N=  321 | ACC= 90.34% | Macro-F1= 89.06%\n",
      "  cid= 3 | N=  182 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 4 | N=  222 | ACC= 97.30% | Macro-F1= 96.54%\n",
      "  cid= 5 | N=  141 | ACC= 99.29% | Macro-F1= 99.18%\n",
      "  cid= 6 | N=  575 | ACC= 92.87% | Macro-F1= 92.73%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 96.22% | Macro-F1= 94.34%\n",
      "  cid= 9 | N=  780 | ACC= 96.03% | Macro-F1= 95.35%\n",
      "  cid=10 | N=  254 | ACC= 98.82% | Macro-F1= 97.48%\n",
      "  cid=11 | N=  254 | ACC= 98.82% | Macro-F1= 98.72%\n",
      "[ROUND 070] VAL(QUERY) Per-Subject ACC = 97.23% Â± 3.00% | Macro-F1 = 96.60% Â± 3.32% | HyperLoss=0.000007 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round071) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.98%  (Std 2.31%)\n",
      "Mean Macro-F1 : 96.10%  (Std 2.82%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.35% | Macro-F1= 96.18%\n",
      "  cid= 2 | N=  321 | ACC= 92.52% | Macro-F1= 91.24%\n",
      "  cid= 3 | N=  182 | ACC= 97.25% | Macro-F1= 95.72%\n",
      "  cid= 4 | N=  222 | ACC= 96.40% | Macro-F1= 95.48%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 95.13% | Macro-F1= 95.08%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 94.75% | Macro-F1= 92.49%\n",
      "  cid= 9 | N=  780 | ACC= 95.51% | Macro-F1= 94.84%\n",
      "  cid=10 | N=  254 | ACC= 97.64% | Macro-F1= 95.15%\n",
      "  cid=11 | N=  254 | ACC= 97.24% | Macro-F1= 97.05%\n",
      "[ROUND 071] VAL(QUERY) Per-Subject ACC = 96.98% Â± 2.31% | Macro-F1 = 96.10% Â± 2.82% | HyperLoss=0.000009 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round072) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.99%  (Std 3.66%)\n",
      "Mean Macro-F1 : 96.39%  (Std 3.89%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 98.11% | Macro-F1= 97.24%\n",
      "  cid= 2 | N=  321 | ACC= 88.47% | Macro-F1= 87.18%\n",
      "  cid= 3 | N=  182 | ACC= 99.45% | Macro-F1= 99.09%\n",
      "  cid= 4 | N=  222 | ACC= 96.85% | Macro-F1= 95.89%\n",
      "  cid= 5 | N=  141 | ACC= 99.29% | Macro-F1= 99.18%\n",
      "  cid= 6 | N=  575 | ACC= 90.96% | Macro-F1= 90.72%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 97.48% | Macro-F1= 96.20%\n",
      "  cid= 9 | N=  780 | ACC= 95.64% | Macro-F1= 94.84%\n",
      "  cid=10 | N=  254 | ACC= 99.21% | Macro-F1= 98.30%\n",
      "  cid=11 | N=  254 | ACC= 98.82% | Macro-F1= 98.72%\n",
      "[ROUND 072] VAL(QUERY) Per-Subject ACC = 96.99% Â± 3.66% | Macro-F1 = 96.39% Â± 3.89% | HyperLoss=0.000007 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round073) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.38%  (Std 2.51%)\n",
      "Mean Macro-F1 : 96.66%  (Std 2.82%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 97.92% | Macro-F1= 97.01%\n",
      "  cid= 2 | N=  321 | ACC= 91.28% | Macro-F1= 89.99%\n",
      "  cid= 3 | N=  182 | ACC= 98.35% | Macro-F1= 97.32%\n",
      "  cid= 4 | N=  222 | ACC= 97.75% | Macro-F1= 97.13%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 94.43% | Macro-F1= 94.37%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 96.64% | Macro-F1= 95.04%\n",
      "  cid= 9 | N=  780 | ACC= 96.15% | Macro-F1= 95.50%\n",
      "  cid=10 | N=  254 | ACC= 98.03% | Macro-F1= 95.91%\n",
      "  cid=11 | N=  254 | ACC= 98.43% | Macro-F1= 98.30%\n",
      "[ROUND 073] VAL(QUERY) Per-Subject ACC = 97.38% Â± 2.51% | Macro-F1 = 96.66% Â± 2.82% | HyperLoss=0.000008 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round074) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.88%  (Std 2.70%)\n",
      "Mean Macro-F1 : 95.99%  (Std 3.23%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.73% | Macro-F1= 96.73%\n",
      "  cid= 2 | N=  321 | ACC= 91.59% | Macro-F1= 90.22%\n",
      "  cid= 3 | N=  182 | ACC= 95.05% | Macro-F1= 92.59%\n",
      "  cid= 4 | N=  222 | ACC= 97.75% | Macro-F1= 97.13%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.57% | Macro-F1= 93.48%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 95.17% | Macro-F1= 93.04%\n",
      "  cid= 9 | N=  780 | ACC= 95.64% | Macro-F1= 94.94%\n",
      "  cid=10 | N=  254 | ACC= 98.03% | Macro-F1= 95.91%\n",
      "  cid=11 | N=  254 | ACC= 98.03% | Macro-F1= 97.88%\n",
      "[ROUND 074] VAL(QUERY) Per-Subject ACC = 96.88% Â± 2.70% | Macro-F1 = 95.99% Â± 3.23% | HyperLoss=0.000007 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round075) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.40%  (Std 2.87%)\n",
      "Mean Macro-F1 : 96.79%  (Std 3.16%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 97.73% | Macro-F1= 96.71%\n",
      "  cid= 2 | N=  321 | ACC= 90.65% | Macro-F1= 89.33%\n",
      "  cid= 3 | N=  182 | ACC= 99.45% | Macro-F1= 99.09%\n",
      "  cid= 4 | N=  222 | ACC= 98.20% | Macro-F1= 97.69%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.39% | Macro-F1= 93.29%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 96.43% | Macro-F1= 94.72%\n",
      "  cid= 9 | N=  780 | ACC= 96.15% | Macro-F1= 95.51%\n",
      "  cid=10 | N=  254 | ACC= 98.82% | Macro-F1= 97.48%\n",
      "  cid=11 | N=  254 | ACC= 98.43% | Macro-F1= 98.30%\n",
      "[ROUND 075] VAL(QUERY) Per-Subject ACC = 97.40% Â± 2.87% | Macro-F1 = 96.79% Â± 3.16% | HyperLoss=0.000005 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round076) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.40%  (Std 2.94%)\n",
      "Mean Macro-F1 : 96.81%  (Std 3.27%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.54% | Macro-F1= 96.42%\n",
      "  cid= 2 | N=  321 | ACC= 90.65% | Macro-F1= 89.33%\n",
      "  cid= 3 | N=  182 | ACC= 99.45% | Macro-F1= 99.11%\n",
      "  cid= 4 | N=  222 | ACC= 97.30% | Macro-F1= 96.50%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.22% | Macro-F1= 93.10%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 96.43% | Macro-F1= 94.72%\n",
      "  cid= 9 | N=  780 | ACC= 96.15% | Macro-F1= 95.51%\n",
      "  cid=10 | N=  254 | ACC= 99.21% | Macro-F1= 98.30%\n",
      "  cid=11 | N=  254 | ACC= 98.82% | Macro-F1= 98.72%\n",
      "[ROUND 076] VAL(QUERY) Per-Subject ACC = 97.40% Â± 2.94% | Macro-F1 = 96.81% Â± 3.27% | HyperLoss=0.000008 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round077) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.38%  (Std 2.67%)\n",
      "Mean Macro-F1 : 96.75%  (Std 2.93%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 97.73% | Macro-F1= 96.71%\n",
      "  cid= 2 | N=  321 | ACC= 91.28% | Macro-F1= 89.99%\n",
      "  cid= 3 | N=  182 | ACC= 98.35% | Macro-F1= 97.32%\n",
      "  cid= 4 | N=  222 | ACC= 97.75% | Macro-F1= 97.10%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.57% | Macro-F1= 93.47%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 96.64% | Macro-F1= 95.04%\n",
      "  cid= 9 | N=  780 | ACC= 96.03% | Macro-F1= 95.36%\n",
      "  cid=10 | N=  254 | ACC= 99.21% | Macro-F1= 98.30%\n",
      "  cid=11 | N=  254 | ACC= 98.43% | Macro-F1= 98.30%\n",
      "[ROUND 077] VAL(QUERY) Per-Subject ACC = 97.38% Â± 2.67% | Macro-F1 = 96.75% Â± 2.93% | HyperLoss=0.000009 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round078) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.15%  (Std 2.37%)\n",
      "Mean Macro-F1 : 96.34%  (Std 2.78%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 97.73% | Macro-F1= 96.73%\n",
      "  cid= 2 | N=  321 | ACC= 91.90% | Macro-F1= 90.56%\n",
      "  cid= 3 | N=  182 | ACC= 97.25% | Macro-F1= 95.72%\n",
      "  cid= 4 | N=  222 | ACC= 98.20% | Macro-F1= 97.72%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 95.13% | Macro-F1= 95.09%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 95.17% | Macro-F1= 93.09%\n",
      "  cid= 9 | N=  780 | ACC= 95.90% | Macro-F1= 95.27%\n",
      "  cid=10 | N=  254 | ACC= 98.03% | Macro-F1= 95.91%\n",
      "  cid=11 | N=  254 | ACC= 96.85% | Macro-F1= 96.64%\n",
      "[ROUND 078] VAL(QUERY) Per-Subject ACC = 97.15% Â± 2.37% | Macro-F1 = 96.34% Â± 2.78% | HyperLoss=0.000008 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round079) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.16%  (Std 3.22%)\n",
      "Mean Macro-F1 : 96.53%  (Std 3.51%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 97.73% | Macro-F1= 96.71%\n",
      "  cid= 2 | N=  321 | ACC= 89.72% | Macro-F1= 88.46%\n",
      "  cid= 3 | N=  182 | ACC= 99.45% | Macro-F1= 99.09%\n",
      "  cid= 4 | N=  222 | ACC= 95.95% | Macro-F1= 94.66%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 92.52% | Macro-F1= 92.37%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 96.85% | Macro-F1= 95.34%\n",
      "  cid= 9 | N=  780 | ACC= 96.03% | Macro-F1= 95.35%\n",
      "  cid=10 | N=  254 | ACC= 99.21% | Macro-F1= 98.30%\n",
      "  cid=11 | N=  254 | ACC= 98.82% | Macro-F1= 98.72%\n",
      "[ROUND 079] VAL(QUERY) Per-Subject ACC = 97.16% Â± 3.22% | Macro-F1 = 96.53% Â± 3.51% | HyperLoss=0.000008 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round080) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.02%  (Std 2.56%)\n",
      "Mean Macro-F1 : 96.14%  (Std 3.01%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.73% | Macro-F1= 96.73%\n",
      "  cid= 2 | N=  321 | ACC= 91.28% | Macro-F1= 89.83%\n",
      "  cid= 3 | N=  182 | ACC= 97.25% | Macro-F1= 95.63%\n",
      "  cid= 4 | N=  222 | ACC= 96.40% | Macro-F1= 95.33%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 94.43% | Macro-F1= 94.38%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 95.80% | Macro-F1= 93.85%\n",
      "  cid= 9 | N=  780 | ACC= 95.64% | Macro-F1= 94.96%\n",
      "  cid=10 | N=  254 | ACC= 97.64% | Macro-F1= 95.15%\n",
      "  cid=11 | N=  254 | ACC= 98.03% | Macro-F1= 97.88%\n",
      "[ROUND 080] VAL(QUERY) Per-Subject ACC = 97.02% Â± 2.56% | Macro-F1 = 96.14% Â± 3.01% | HyperLoss=0.000009 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round081) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.19%  (Std 3.02%)\n",
      "Mean Macro-F1 : 96.55%  (Std 3.29%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 97.54% | Macro-F1= 96.44%\n",
      "  cid= 2 | N=  321 | ACC= 90.65% | Macro-F1= 89.44%\n",
      "  cid= 3 | N=  182 | ACC= 99.45% | Macro-F1= 99.09%\n",
      "  cid= 4 | N=  222 | ACC= 95.50% | Macro-F1= 94.03%\n",
      "  cid= 5 | N=  141 | ACC= 99.29% | Macro-F1= 99.18%\n",
      "  cid= 6 | N=  575 | ACC= 92.52% | Macro-F1= 92.39%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 97.27% | Macro-F1= 95.93%\n",
      "  cid= 9 | N=  780 | ACC= 96.03% | Macro-F1= 95.34%\n",
      "  cid=10 | N=  254 | ACC= 99.21% | Macro-F1= 98.30%\n",
      "  cid=11 | N=  254 | ACC= 99.21% | Macro-F1= 99.15%\n",
      "[ROUND 081] VAL(QUERY) Per-Subject ACC = 97.19% Â± 3.02% | Macro-F1 = 96.55% Â± 3.29% | HyperLoss=0.000011 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round082) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.46%  (Std 2.82%)\n",
      "Mean Macro-F1 : 96.83%  (Std 3.12%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 97.73% | Macro-F1= 96.73%\n",
      "  cid= 2 | N=  321 | ACC= 90.97% | Macro-F1= 89.71%\n",
      "  cid= 3 | N=  182 | ACC= 99.45% | Macro-F1= 99.09%\n",
      "  cid= 4 | N=  222 | ACC= 98.65% | Macro-F1= 98.28%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.39% | Macro-F1= 93.30%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 96.43% | Macro-F1= 94.72%\n",
      "  cid= 9 | N=  780 | ACC= 96.03% | Macro-F1= 95.40%\n",
      "  cid=10 | N=  254 | ACC= 98.43% | Macro-F1= 96.69%\n",
      "  cid=11 | N=  254 | ACC= 98.82% | Macro-F1= 98.72%\n",
      "[ROUND 082] VAL(QUERY) Per-Subject ACC = 97.46% Â± 2.82% | Macro-F1 = 96.83% Â± 3.12% | HyperLoss=0.000008 | p_rate=0.591\n",
      "ðŸ”¥ NEW BEST (by VAL/QUERY) @ Round 82 | Acc=97.46% | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round083) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.26%  (Std 2.74%)\n",
      "Mean Macro-F1 : 96.56%  (Std 3.02%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 97.54% | Macro-F1= 96.44%\n",
      "  cid= 2 | N=  321 | ACC= 91.59% | Macro-F1= 90.33%\n",
      "  cid= 3 | N=  182 | ACC= 98.35% | Macro-F1= 97.32%\n",
      "  cid= 4 | N=  222 | ACC= 98.65% | Macro-F1= 98.28%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 92.70% | Macro-F1= 92.58%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 96.01% | Macro-F1= 94.18%\n",
      "  cid= 9 | N=  780 | ACC= 95.90% | Macro-F1= 95.24%\n",
      "  cid=10 | N=  254 | ACC= 98.43% | Macro-F1= 96.69%\n",
      "  cid=11 | N=  254 | ACC= 98.43% | Macro-F1= 98.30%\n",
      "[ROUND 083] VAL(QUERY) Per-Subject ACC = 97.26% Â± 2.74% | Macro-F1 = 96.56% Â± 3.02% | HyperLoss=0.000011 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round084) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.24%  (Std 2.78%)\n",
      "Mean Macro-F1 : 96.53%  (Std 3.15%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 97.92% | Macro-F1= 96.99%\n",
      "  cid= 2 | N=  321 | ACC= 90.97% | Macro-F1= 89.71%\n",
      "  cid= 3 | N=  182 | ACC= 98.90% | Macro-F1= 98.19%\n",
      "  cid= 4 | N=  222 | ACC= 97.30% | Macro-F1= 96.50%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.91% | Macro-F1= 93.83%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 94.96% | Macro-F1= 92.67%\n",
      "  cid= 9 | N=  780 | ACC= 96.03% | Macro-F1= 95.35%\n",
      "  cid=10 | N=  254 | ACC= 98.82% | Macro-F1= 97.48%\n",
      "  cid=11 | N=  254 | ACC= 98.43% | Macro-F1= 98.30%\n",
      "[ROUND 084] VAL(QUERY) Per-Subject ACC = 97.24% Â± 2.78% | Macro-F1 = 96.53% Â± 3.15% | HyperLoss=0.000009 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round085) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.05%  (Std 2.69%)\n",
      "Mean Macro-F1 : 96.29%  (Std 3.15%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.35% | Macro-F1= 96.16%\n",
      "  cid= 2 | N=  321 | ACC= 91.90% | Macro-F1= 90.51%\n",
      "  cid= 3 | N=  182 | ACC= 98.90% | Macro-F1= 98.23%\n",
      "  cid= 4 | N=  222 | ACC= 97.30% | Macro-F1= 96.57%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.74% | Macro-F1= 93.64%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 94.12% | Macro-F1= 91.67%\n",
      "  cid= 9 | N=  780 | ACC= 95.64% | Macro-F1= 94.96%\n",
      "  cid=10 | N=  254 | ACC= 98.43% | Macro-F1= 96.69%\n",
      "  cid=11 | N=  254 | ACC= 97.24% | Macro-F1= 97.05%\n",
      "[ROUND 085] VAL(QUERY) Per-Subject ACC = 97.05% Â± 2.69% | Macro-F1 = 96.29% Â± 3.15% | HyperLoss=0.000009 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round086) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.20%  (Std 3.23%)\n",
      "Mean Macro-F1 : 96.59%  (Std 3.45%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 97.73% | Macro-F1= 96.69%\n",
      "  cid= 2 | N=  321 | ACC= 89.72% | Macro-F1= 88.51%\n",
      "  cid= 3 | N=  182 | ACC= 99.45% | Macro-F1= 99.09%\n",
      "  cid= 4 | N=  222 | ACC= 96.85% | Macro-F1= 95.89%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 92.17% | Macro-F1= 92.03%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 97.27% | Macro-F1= 95.96%\n",
      "  cid= 9 | N=  780 | ACC= 96.03% | Macro-F1= 95.32%\n",
      "  cid=10 | N=  254 | ACC= 98.82% | Macro-F1= 97.48%\n",
      "  cid=11 | N=  254 | ACC= 98.82% | Macro-F1= 98.72%\n",
      "[ROUND 086] VAL(QUERY) Per-Subject ACC = 97.20% Â± 3.23% | Macro-F1 = 96.59% Â± 3.45% | HyperLoss=0.000010 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round087) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.31%  (Std 3.00%)\n",
      "Mean Macro-F1 : 96.71%  (Std 3.34%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 97.35% | Macro-F1= 96.16%\n",
      "  cid= 2 | N=  321 | ACC= 90.65% | Macro-F1= 89.38%\n",
      "  cid= 3 | N=  182 | ACC= 99.45% | Macro-F1= 99.09%\n",
      "  cid= 4 | N=  222 | ACC= 97.30% | Macro-F1= 96.50%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 92.87% | Macro-F1= 92.74%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 96.22% | Macro-F1= 94.26%\n",
      "  cid= 9 | N=  780 | ACC= 95.90% | Macro-F1= 95.16%\n",
      "  cid=10 | N=  254 | ACC= 99.61% | Macro-F1= 99.14%\n",
      "  cid=11 | N=  254 | ACC= 98.82% | Macro-F1= 98.72%\n",
      "[ROUND 087] VAL(QUERY) Per-Subject ACC = 97.31% Â± 3.00% | Macro-F1 = 96.71% Â± 3.34% | HyperLoss=0.000009 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round088) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.14%  (Std 2.98%)\n",
      "Mean Macro-F1 : 96.40%  (Std 3.44%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.54% | Macro-F1= 96.44%\n",
      "  cid= 2 | N=  321 | ACC= 90.65% | Macro-F1= 89.28%\n",
      "  cid= 3 | N=  182 | ACC= 99.45% | Macro-F1= 99.09%\n",
      "  cid= 4 | N=  222 | ACC= 98.20% | Macro-F1= 97.72%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.22% | Macro-F1= 93.13%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 94.96% | Macro-F1= 92.72%\n",
      "  cid= 9 | N=  780 | ACC= 95.64% | Macro-F1= 94.94%\n",
      "  cid=10 | N=  254 | ACC= 97.64% | Macro-F1= 95.15%\n",
      "  cid=11 | N=  254 | ACC= 98.43% | Macro-F1= 98.30%\n",
      "[ROUND 088] VAL(QUERY) Per-Subject ACC = 97.14% Â± 2.98% | Macro-F1 = 96.40% Â± 3.44% | HyperLoss=0.000008 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round089) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.25%  (Std 2.77%)\n",
      "Mean Macro-F1 : 96.59%  (Std 3.12%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 97.35% | Macro-F1= 96.16%\n",
      "  cid= 2 | N=  321 | ACC= 90.97% | Macro-F1= 89.66%\n",
      "  cid= 3 | N=  182 | ACC= 98.90% | Macro-F1= 98.19%\n",
      "  cid= 4 | N=  222 | ACC= 97.75% | Macro-F1= 97.10%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.39% | Macro-F1= 93.29%\n",
      "  cid= 7 | N=  113 | ACC= 99.12% | Macro-F1= 99.11%\n",
      "  cid= 8 | N=  476 | ACC= 95.38% | Macro-F1= 93.19%\n",
      "  cid= 9 | N=  780 | ACC= 96.54% | Macro-F1= 95.96%\n",
      "  cid=10 | N=  254 | ACC= 99.21% | Macro-F1= 98.30%\n",
      "  cid=11 | N=  254 | ACC= 98.82% | Macro-F1= 98.72%\n",
      "[ROUND 089] VAL(QUERY) Per-Subject ACC = 97.25% Â± 2.77% | Macro-F1 = 96.59% Â± 3.12% | HyperLoss=0.000008 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round090) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.36%  (Std 2.71%)\n",
      "Mean Macro-F1 : 96.71%  (Std 3.15%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.35% | Macro-F1= 96.16%\n",
      "  cid= 2 | N=  321 | ACC= 92.21% | Macro-F1= 90.90%\n",
      "  cid= 3 | N=  182 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 4 | N=  222 | ACC= 98.20% | Macro-F1= 97.72%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.39% | Macro-F1= 93.29%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 94.96% | Macro-F1= 92.81%\n",
      "  cid= 9 | N=  780 | ACC= 95.77% | Macro-F1= 95.09%\n",
      "  cid=10 | N=  254 | ACC= 98.43% | Macro-F1= 96.69%\n",
      "  cid=11 | N=  254 | ACC= 98.03% | Macro-F1= 97.88%\n",
      "[ROUND 090] VAL(QUERY) Per-Subject ACC = 97.36% Â± 2.71% | Macro-F1 = 96.71% Â± 3.15% | HyperLoss=0.000010 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round091) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.34%  (Std 3.11%)\n",
      "Mean Macro-F1 : 96.74%  (Std 3.33%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 97.92% | Macro-F1= 96.97%\n",
      "  cid= 2 | N=  321 | ACC= 90.34% | Macro-F1= 89.21%\n",
      "  cid= 3 | N=  182 | ACC= 98.90% | Macro-F1= 98.15%\n",
      "  cid= 4 | N=  222 | ACC= 97.75% | Macro-F1= 97.13%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 92.17% | Macro-F1= 91.99%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 96.64% | Macro-F1= 95.04%\n",
      "  cid= 9 | N=  780 | ACC= 96.28% | Macro-F1= 95.62%\n",
      "  cid=10 | N=  254 | ACC= 99.21% | Macro-F1= 98.30%\n",
      "  cid=11 | N=  254 | ACC= 99.21% | Macro-F1= 99.15%\n",
      "[ROUND 091] VAL(QUERY) Per-Subject ACC = 97.34% Â± 3.11% | Macro-F1 = 96.74% Â± 3.33% | HyperLoss=0.000009 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round092) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.38%  (Std 2.72%)\n",
      "Mean Macro-F1 : 96.71%  (Std 3.16%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.73% | Macro-F1= 96.71%\n",
      "  cid= 2 | N=  321 | ACC= 91.28% | Macro-F1= 89.94%\n",
      "  cid= 3 | N=  182 | ACC= 98.35% | Macro-F1= 97.38%\n",
      "  cid= 4 | N=  222 | ACC= 97.75% | Macro-F1= 97.10%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 94.26% | Macro-F1= 94.18%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 94.96% | Macro-F1= 92.67%\n",
      "  cid= 9 | N=  780 | ACC= 96.15% | Macro-F1= 95.56%\n",
      "  cid=10 | N=  254 | ACC= 99.21% | Macro-F1= 98.30%\n",
      "  cid=11 | N=  254 | ACC= 98.82% | Macro-F1= 98.72%\n",
      "[ROUND 092] VAL(QUERY) Per-Subject ACC = 97.38% Â± 2.72% | Macro-F1 = 96.71% Â± 3.16% | HyperLoss=0.000008 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round093) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.39%  (Std 2.37%)\n",
      "Mean Macro-F1 : 96.61%  (Std 2.77%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 97.54% | Macro-F1= 96.44%\n",
      "  cid= 2 | N=  321 | ACC= 92.83% | Macro-F1= 91.67%\n",
      "  cid= 3 | N=  182 | ACC= 97.80% | Macro-F1= 96.46%\n",
      "  cid= 4 | N=  222 | ACC= 98.65% | Macro-F1= 98.30%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.91% | Macro-F1= 93.84%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 95.17% | Macro-F1= 93.00%\n",
      "  cid= 9 | N=  780 | ACC= 96.28% | Macro-F1= 95.68%\n",
      "  cid=10 | N=  254 | ACC= 98.03% | Macro-F1= 95.91%\n",
      "  cid=11 | N=  254 | ACC= 98.82% | Macro-F1= 98.72%\n",
      "[ROUND 093] VAL(QUERY) Per-Subject ACC = 97.39% Â± 2.37% | Macro-F1 = 96.61% Â± 2.77% | HyperLoss=0.000007 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round094) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.97%  (Std 3.52%)\n",
      "Mean Macro-F1 : 96.29%  (Std 3.80%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.19% | Macro-F1= 98.69%\n",
      "  cid= 1 | N=  529 | ACC= 97.92% | Macro-F1= 96.95%\n",
      "  cid= 2 | N=  321 | ACC= 88.79% | Macro-F1= 87.55%\n",
      "  cid= 3 | N=  182 | ACC= 99.45% | Macro-F1= 99.09%\n",
      "  cid= 4 | N=  222 | ACC= 95.50% | Macro-F1= 94.03%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 91.65% | Macro-F1= 91.46%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 96.43% | Macro-F1= 94.60%\n",
      "  cid= 9 | N=  780 | ACC= 96.67% | Macro-F1= 96.07%\n",
      "  cid=10 | N=  254 | ACC= 99.21% | Macro-F1= 98.30%\n",
      "  cid=11 | N=  254 | ACC= 98.82% | Macro-F1= 98.72%\n",
      "[ROUND 094] VAL(QUERY) Per-Subject ACC = 96.97% Â± 3.52% | Macro-F1 = 96.29% Â± 3.80% | HyperLoss=0.000008 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round095) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.08%  (Std 2.53%)\n",
      "Mean Macro-F1 : 96.26%  (Std 2.95%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 97.92% | Macro-F1= 96.99%\n",
      "  cid= 2 | N=  321 | ACC= 91.59% | Macro-F1= 90.27%\n",
      "  cid= 3 | N=  182 | ACC= 96.70% | Macro-F1= 94.81%\n",
      "  cid= 4 | N=  222 | ACC= 98.20% | Macro-F1= 97.69%\n",
      "  cid= 5 | N=  141 | ACC= 99.29% | Macro-F1= 99.18%\n",
      "  cid= 6 | N=  575 | ACC= 94.26% | Macro-F1= 94.19%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 94.96% | Macro-F1= 92.72%\n",
      "  cid= 9 | N=  780 | ACC= 95.64% | Macro-F1= 94.94%\n",
      "  cid=10 | N=  254 | ACC= 98.43% | Macro-F1= 96.69%\n",
      "  cid=11 | N=  254 | ACC= 98.43% | Macro-F1= 98.30%\n",
      "[ROUND 095] VAL(QUERY) Per-Subject ACC = 97.08% Â± 2.53% | Macro-F1 = 96.26% Â± 2.95% | HyperLoss=0.000011 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round096) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.54%  (Std 2.23%)\n",
      "Mean Macro-F1 : 96.83%  (Std 2.59%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.54% | Macro-F1= 96.44%\n",
      "  cid= 2 | N=  321 | ACC= 92.83% | Macro-F1= 91.67%\n",
      "  cid= 3 | N=  182 | ACC= 98.35% | Macro-F1= 97.38%\n",
      "  cid= 4 | N=  222 | ACC= 97.75% | Macro-F1= 97.10%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 94.61% | Macro-F1= 94.55%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 96.22% | Macro-F1= 94.46%\n",
      "  cid= 9 | N=  780 | ACC= 96.28% | Macro-F1= 95.69%\n",
      "  cid=10 | N=  254 | ACC= 98.03% | Macro-F1= 95.91%\n",
      "  cid=11 | N=  254 | ACC= 98.82% | Macro-F1= 98.72%\n",
      "[ROUND 096] VAL(QUERY) Per-Subject ACC = 97.54% Â± 2.23% | Macro-F1 = 96.83% Â± 2.59% | HyperLoss=0.000008 | p_rate=0.591\n",
      "ðŸ”¥ NEW BEST (by VAL/QUERY) @ Round 96 | Acc=97.54% | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round097) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.08%  (Std 3.09%)\n",
      "Mean Macro-F1 : 96.36%  (Std 3.35%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC= 99.60% | Macro-F1= 99.35%\n",
      "  cid= 1 | N=  529 | ACC= 97.16% | Macro-F1= 95.87%\n",
      "  cid= 2 | N=  321 | ACC= 89.72% | Macro-F1= 88.46%\n",
      "  cid= 3 | N=  182 | ACC= 97.80% | Macro-F1= 96.46%\n",
      "  cid= 4 | N=  222 | ACC= 97.75% | Macro-F1= 97.10%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 92.70% | Macro-F1= 92.58%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 96.43% | Macro-F1= 94.75%\n",
      "  cid= 9 | N=  780 | ACC= 96.15% | Macro-F1= 95.48%\n",
      "  cid=10 | N=  254 | ACC= 98.82% | Macro-F1= 97.48%\n",
      "  cid=11 | N=  254 | ACC= 98.82% | Macro-F1= 98.72%\n",
      "[ROUND 097] VAL(QUERY) Per-Subject ACC = 97.08% Â± 3.09% | Macro-F1 = 96.36% Â± 3.35% | HyperLoss=0.000006 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round098) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.26%  (Std 2.41%)\n",
      "Mean Macro-F1 : 96.56%  (Std 2.76%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.54% | Macro-F1= 96.47%\n",
      "  cid= 2 | N=  321 | ACC= 91.59% | Macro-F1= 90.37%\n",
      "  cid= 3 | N=  182 | ACC= 98.90% | Macro-F1= 98.23%\n",
      "  cid= 4 | N=  222 | ACC= 97.75% | Macro-F1= 97.13%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 94.43% | Macro-F1= 94.38%\n",
      "  cid= 7 | N=  113 | ACC= 98.23% | Macro-F1= 98.22%\n",
      "  cid= 8 | N=  476 | ACC= 95.80% | Macro-F1= 93.89%\n",
      "  cid= 9 | N=  780 | ACC= 96.41% | Macro-F1= 95.83%\n",
      "  cid=10 | N=  254 | ACC= 98.03% | Macro-F1= 95.91%\n",
      "  cid=11 | N=  254 | ACC= 98.43% | Macro-F1= 98.30%\n",
      "[ROUND 098] VAL(QUERY) Per-Subject ACC = 97.26% Â± 2.41% | Macro-F1 = 96.56% Â± 2.76% | HyperLoss=0.000006 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round099) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 97.44%  (Std 3.18%)\n",
      "Mean Macro-F1 : 96.87%  (Std 3.52%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.73% | Macro-F1= 96.73%\n",
      "  cid= 2 | N=  321 | ACC= 89.41% | Macro-F1= 88.14%\n",
      "  cid= 3 | N=  182 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 4 | N=  222 | ACC= 97.30% | Macro-F1= 96.50%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 93.74% | Macro-F1= 93.63%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 96.64% | Macro-F1= 95.01%\n",
      "  cid= 9 | N=  780 | ACC= 96.41% | Macro-F1= 95.79%\n",
      "  cid=10 | N=  254 | ACC= 98.82% | Macro-F1= 97.48%\n",
      "  cid=11 | N=  254 | ACC= 99.21% | Macro-F1= 99.15%\n",
      "[ROUND 099] VAL(QUERY) Per-Subject ACC = 97.44% Â± 3.18% | Macro-F1 = 96.87% Â± 3.52% | HyperLoss=0.000010 | p_rate=0.591\n",
      "\n",
      "================ PFML INTRA (VAL@Round100) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 96.97%  (Std 2.68%)\n",
      "Mean Macro-F1 : 96.16%  (Std 3.18%)\n",
      "====================================================\n",
      "\n",
      "Per-subject results:\n",
      "  cid= 0 | N=  247 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 1 | N=  529 | ACC= 97.73% | Macro-F1= 96.73%\n",
      "  cid= 2 | N=  321 | ACC= 91.59% | Macro-F1= 90.33%\n",
      "  cid= 3 | N=  182 | ACC= 96.15% | Macro-F1= 94.12%\n",
      "  cid= 4 | N=  222 | ACC= 97.30% | Macro-F1= 96.50%\n",
      "  cid= 5 | N=  141 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 6 | N=  575 | ACC= 94.26% | Macro-F1= 94.21%\n",
      "  cid= 7 | N=  113 | ACC=100.00% | Macro-F1=100.00%\n",
      "  cid= 8 | N=  476 | ACC= 94.33% | Macro-F1= 91.99%\n",
      "  cid= 9 | N=  780 | ACC= 95.38% | Macro-F1= 94.68%\n",
      "  cid=10 | N=  254 | ACC= 98.82% | Macro-F1= 97.48%\n",
      "  cid=11 | N=  254 | ACC= 98.03% | Macro-F1= 97.88%\n",
      "[ROUND 100] VAL(QUERY) Per-Subject ACC = 96.97% Â± 2.68% | Macro-F1 = 96.16% Â± 3.18% | HyperLoss=0.000008 | p_rate=0.591\n",
      "\n",
      "================ STAGE-2 BEST SUMMARY ================\n",
      "Best VAL(QUERY) Accuracy : 97.54%\n",
      "Best Round               : 96\n",
      "Best p_rate              : 0.591\n",
      "Saved checkpoints:\n",
      " - PFML_IC2ASTER_pruned50_bestVAL_global.pth\n",
      " - PFML_IC2ASTER_pruned50_bestVAL_hypernet.pth\n",
      " - PFML_IC2ASTER_pruned50_bestVAL_embed.pth\n",
      " - PFML_IC2ASTER_pruned50_bestVAL_meta.json\n",
      "======================================================\n",
      "\n",
      "Saved final:\n",
      " - PFML_IC2ASTER_pruned50_final_global.pth\n",
      " - PFML_IC2ASTER_pruned50_final_hypernet.pth\n",
      " - PFML_IC2ASTER_pruned50_final_embed.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 7) Stage-2 PFML retraining (VAL = QUERY)\n",
    "# ============================================================\n",
    "save_prefix = f\"PFML_IC2ASTER_pruned{int(PRUNE_RATIO * 100)}\"\n",
    "\n",
    "trainer = PFMLTrainer(\n",
    "    num_client=num_client,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    qr_split_rate=QR_SPLIT_RATE,\n",
    "    innerlr=INNER_LR,\n",
    "    outterlr=OUTTER_LR,\n",
    "    innerstep=INNER_STEP,\n",
    "    outterstep=OUTTER_STEP,\n",
    "    p_rate=p_rate_cap,\n",
    "    ff_dim=model_pruned.ff_dim,\n",
    "    class_weights=class_weights,\n",
    "    init_global_state=copy.deepcopy(model_pruned.state_dict()),\n",
    "    init_hyper_state=hyper_state,\n",
    "    init_embed_state=embed_state,\n",
    ")\n",
    "\n",
    "trainer.init_clients(sup_sets, que_sets, test_sets)\n",
    "trainer.run(NUM_ROUNDS_STAGE2, save_prefix=save_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6d03018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_pfml_history(trainer, save_dir=None):\n",
    "    h = trainer.history\n",
    "    r = np.array(h[\"round\"], dtype=np.int32)\n",
    "\n",
    "    if save_dir is not None:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # ============================================================\n",
    "    # 1) VAL metrics + p_rate in ONE figure (dual y-axes)\n",
    "    #    - Left y: ACC/F1 (%)\n",
    "    #    - Right y: p_rate\n",
    "    # ============================================================\n",
    "    fig = plt.figure()\n",
    "    ax1 = plt.gca()\n",
    "\n",
    "    # --- make VAL curves prominent ---\n",
    "    ax1.plot(\n",
    "        r, np.array(h[\"val_acc_mean\"]) * 100.0,\n",
    "        linestyle='-', linewidth=2.2,\n",
    "        label=\"VAL ACC\",\n",
    "        zorder=3\n",
    "    )\n",
    "    ax1.plot(\n",
    "        r, np.array(h[\"val_f1_mean\"]) * 100.0,\n",
    "        linestyle='--', linewidth=2.2,\n",
    "        label=\"VAL Macro-F1\",\n",
    "        zorder=3\n",
    "    )\n",
    "\n",
    "    ax1.set_xlabel(\"Round\")\n",
    "    ax1.set_ylabel(\"VAL(QUERY) (%)\")\n",
    "    ax1.set_xlim(left=0)\n",
    "    ax1.set_ylim(0, 100)\n",
    "\n",
    "    # --- p_rate on right axis: lighter + behind ---\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(\n",
    "        r, np.array(h[\"p_rate\"]),\n",
    "        marker='^', markersize=4,\n",
    "        linestyle='-', linewidth=1.6,\n",
    "        alpha=0.55,          # <-- lighter so it won't \"cover\" visually\n",
    "        label=\"p_rate\",\n",
    "        zorder=1             # <-- put behind\n",
    "    )\n",
    "    ax2.set_ylabel(\"p_rate\")\n",
    "    ax2.set_ylim(0, 1)\n",
    "\n",
    "    # combined legend (place to avoid overlapping curves)\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc=\"lower right\")\n",
    "\n",
    "    plt.title(\"PFML validation metrics with personalization schedule (SAD)- Stage 2\", pad=14)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    if save_dir is not None:\n",
    "        plt.savefig(os.path.join(save_dir, \"pfml_val_acc_f1_prate.png\"),\n",
    "                    dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    # # ============================================================\n",
    "    # # 2) Training dynamics (Inner/Outer + Hyper) in ONE figure\n",
    "    # # ============================================================\n",
    "    # fig = plt.figure()\n",
    "    # ax1 = plt.gca()\n",
    "\n",
    "    # ax1.plot(\n",
    "    #     r, np.array(h[\"inner_loss_mean\"]),\n",
    "    #     marker='o', markersize=4,\n",
    "    #     linestyle='-', linewidth=2.0,\n",
    "    #     label=\"Inner Loss (Support)\",\n",
    "    #     zorder=3\n",
    "    # )\n",
    "    # ax1.plot(\n",
    "    #     r, np.array(h[\"outer_loss_mean\"]),\n",
    "    #     marker='s', markersize=4,\n",
    "    #     linestyle='-', linewidth=2.0,\n",
    "    #     label=\"Outer Loss (Query)\",\n",
    "    #     zorder=3\n",
    "    # )\n",
    "    # ax1.set_xlabel(\"Round\")\n",
    "    # ax1.set_ylabel(\"Loss (CE)\")\n",
    "    # ax1.set_xlim(left=0)\n",
    "    # ax1.set_ylim(bottom=0)\n",
    "\n",
    "    # ax2 = ax1.twinx()\n",
    "    # ax2.plot(\n",
    "    #     r, np.array(h[\"hyper_loss\"]),\n",
    "    #     linestyle='-.', linewidth=1.6,\n",
    "    #     alpha=0.7,\n",
    "    #     label=\"Hyper Loss (MSE)\",\n",
    "    #     zorder=1\n",
    "    # )\n",
    "    # ax2.set_ylabel(\"Hyper Loss (MSE)\")\n",
    "    # ax2.set_ylim(bottom=0)\n",
    "\n",
    "    # lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    # lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    # ax1.legend(lines1 + lines2, labels1 + labels2, loc=\"upper right\")\n",
    "\n",
    "    # plt.title(\"PFML Training Dynamics\")\n",
    "    # plt.tight_layout()\n",
    "    # if save_dir is not None:\n",
    "    #     plt.savefig(os.path.join(save_dir, \"pfml_losses_combined.png\"),\n",
    "    #                 dpi=200, bbox_inches=\"tight\")\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e687fabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user00\\AppData\\Local\\Temp\\ipykernel_69196\\59190286.py:60: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout(rect=[0, 0, 1, 0.95])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_pfml_history(trainer, save_dir=\"pfml_plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78488330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEST META] best_round=96 best_val_acc=0.9753637313842773 best_p_rate=0.591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user00\\AppData\\Local\\Temp\\ipykernel_69196\\3438542971.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(best_global_path, map_location=DEVICE),\n",
      "C:\\Users\\user00\\AppData\\Local\\Temp\\ipykernel_69196\\3438542971.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(best_hyper_path, map_location=DEVICE),\n",
      "C:\\Users\\user00\\AppData\\Local\\Temp\\ipykernel_69196\\3438542971.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(best_embed_path, map_location=DEVICE),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ PFML FINAL_TEST (TEST) ================\n",
      "Subjects used : 12\n",
      "Mean ACC      : 95.34%  (Std 1.80%)\n",
      "Mean Macro-F1 : 93.29%  (Std 3.27%)\n",
      "Overall Confusion Matrix (all test samples):\n",
      "[[1255   94]\n",
      " [ 130 2913]]\n",
      "\n",
      "Per-subject results:\n",
      "  cid |     N |   ACC   | Macro-F1\n",
      "------|-------|---------|---------\n",
      "    0 |   265 |  96.98% |   94.99%\n",
      "    1 |   567 |  95.41% |   93.04%\n",
      "    2 |   344 |  93.90% |   92.24%\n",
      "    3 |   195 |  94.87% |   91.48%\n",
      "    4 |   239 |  96.65% |   96.02%\n",
      "    5 |   152 |  97.37% |   97.08%\n",
      "    6 |   617 |  91.90% |   91.86%\n",
      "    7 |   121 |  98.35% |   98.27%\n",
      "    8 |   510 |  94.51% |   90.39%\n",
      "    9 |   836 |  95.81% |   94.90%\n",
      "   10 |   273 |  94.51% |   86.16%\n",
      "   11 |   273 |  93.77% |   93.09%\n",
      "====================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([{'cid': 0,\n",
       "   'n': 265,\n",
       "   'acc': 0.969811320754717,\n",
       "   'macro_f1': 0.9499244142101286,\n",
       "   'cm': array([[ 45,   5],\n",
       "          [  3, 212]], dtype=int64)},\n",
       "  {'cid': 1,\n",
       "   'n': 567,\n",
       "   'acc': 0.9541446208112875,\n",
       "   'macro_f1': 0.9304386395379562,\n",
       "   'cm': array([[105,  18],\n",
       "          [  8, 436]], dtype=int64)},\n",
       "  {'cid': 2,\n",
       "   'n': 344,\n",
       "   'acc': 0.938953488372093,\n",
       "   'macro_f1': 0.9223684917521895,\n",
       "   'cm': array([[241,   8],\n",
       "          [ 13,  82]], dtype=int64)},\n",
       "  {'cid': 3,\n",
       "   'n': 195,\n",
       "   'acc': 0.9487179487179487,\n",
       "   'macro_f1': 0.914832285115304,\n",
       "   'cm': array([[ 31,   0],\n",
       "          [ 10, 154]], dtype=int64)},\n",
       "  {'cid': 4,\n",
       "   'n': 239,\n",
       "   'acc': 0.9665271966527197,\n",
       "   'macro_f1': 0.9602461743180306,\n",
       "   'cm': array([[ 68,   3],\n",
       "          [  5, 163]], dtype=int64)},\n",
       "  {'cid': 5,\n",
       "   'n': 152,\n",
       "   'acc': 0.9736842105263158,\n",
       "   'macro_f1': 0.9707692307692307,\n",
       "   'cm': array([[50,  4],\n",
       "          [ 0, 98]], dtype=int64)},\n",
       "  {'cid': 6,\n",
       "   'n': 617,\n",
       "   'acc': 0.9189627228525121,\n",
       "   'macro_f1': 0.9185672051525711,\n",
       "   'cm': array([[262,  33],\n",
       "          [ 17, 305]], dtype=int64)},\n",
       "  {'cid': 7,\n",
       "   'n': 121,\n",
       "   'acc': 0.9834710743801653,\n",
       "   'macro_f1': 0.9827340182648402,\n",
       "   'cm': array([[47,  0],\n",
       "          [ 2, 72]], dtype=int64)},\n",
       "  {'cid': 8,\n",
       "   'n': 510,\n",
       "   'acc': 0.9450980392156862,\n",
       "   'macro_f1': 0.9038668677294268,\n",
       "   'cm': array([[ 74,   5],\n",
       "          [ 23, 408]], dtype=int64)},\n",
       "  {'cid': 9,\n",
       "   'n': 836,\n",
       "   'acc': 0.9581339712918661,\n",
       "   'macro_f1': 0.9490498653112468,\n",
       "   'cm': array([[224,  13],\n",
       "          [ 22, 577]], dtype=int64)},\n",
       "  {'cid': 10,\n",
       "   'n': 273,\n",
       "   'acc': 0.945054945054945,\n",
       "   'macro_f1': 0.8615852628020957,\n",
       "   'cm': array([[ 23,   3],\n",
       "          [ 12, 235]], dtype=int64)},\n",
       "  {'cid': 11,\n",
       "   'n': 273,\n",
       "   'acc': 0.9377289377289377,\n",
       "   'macro_f1': 0.9308685743226134,\n",
       "   'cm': array([[ 85,   2],\n",
       "          [ 15, 171]], dtype=int64)}],\n",
       " (0.9533574, 0.01804291),\n",
       " (0.93293756, 0.032749236),\n",
       " array([[1255,   94],\n",
       "        [ 130, 2913]], dtype=int64))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 8) FINAL TEST (load best VAL checkpoint)\n",
    "# ============================================================\n",
    "best_meta_path   = f\"{save_prefix}_bestVAL_meta.json\"\n",
    "best_global_path = f\"{save_prefix}_bestVAL_global.pth\"\n",
    "best_hyper_path  = f\"{save_prefix}_bestVAL_hypernet.pth\"\n",
    "best_embed_path  = f\"{save_prefix}_bestVAL_embed.pth\"\n",
    "\n",
    "best_p_rate = P_RATE\n",
    "if os.path.exists(best_meta_path):\n",
    "    import json\n",
    "    with open(best_meta_path, \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "    best_p_rate = float(meta.get(\"best_p_rate\", P_RATE))\n",
    "    print(\n",
    "        f\"[BEST META] best_round={meta.get('best_round')} \"\n",
    "        f\"best_val_acc={meta.get('best_val_acc')} \"\n",
    "        f\"best_p_rate={best_p_rate}\"\n",
    "    )\n",
    "\n",
    "if os.path.exists(best_global_path):\n",
    "    trainer.global_model.load_state_dict(\n",
    "        torch.load(best_global_path, map_location=DEVICE),\n",
    "        strict=True\n",
    "    )\n",
    "if os.path.exists(best_hyper_path):\n",
    "    trainer.hypernet.load_state_dict(\n",
    "        torch.load(best_hyper_path, map_location=DEVICE),\n",
    "        strict=True\n",
    "    )\n",
    "if os.path.exists(best_embed_path):\n",
    "    trainer.embed.load_state_dict(\n",
    "        torch.load(best_embed_path, map_location=DEVICE),\n",
    "        strict=True\n",
    "    )\n",
    "\n",
    "# ============================================================\n",
    "# 9) TEST once (paper-grade)\n",
    "# ============================================================\n",
    "eval_test_per_subject_pfml(\n",
    "    trainer,\n",
    "    p_rate=best_p_rate,\n",
    "    tag=\"FINAL_TEST\",\n",
    "    print_per_subject=True,\n",
    "    save_txt=True,\n",
    "    out_path=f\"{save_prefix}_FINAL_TEST_detail.txt\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ducenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
